<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Generic interface · CUDSS.jl</title><meta name="title" content="Generic interface · CUDSS.jl"/><meta property="og:title" content="Generic interface · CUDSS.jl"/><meta property="twitter:title" content="Generic interface · CUDSS.jl"/><meta name="description" content="Documentation for CUDSS.jl."/><meta property="og:description" content="Documentation for CUDSS.jl."/><meta property="twitter:description" content="Documentation for CUDSS.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">CUDSS.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../functions/">Functions</a></li><li><a class="tocitem" href="../cudss/">cuDSS interface</a></li><li class="is-active"><a class="tocitem" href>Generic interface</a><ul class="internal"><li><a class="tocitem" href="#LLᵀ-and-LLᴴ"><span>LLᵀ and LLᴴ</span></a></li><li><a class="tocitem" href="#LDLᵀ-and-LDLᴴ"><span>LDLᵀ and LDLᴴ</span></a></li><li><a class="tocitem" href="#LU"><span>LU</span></a></li></ul></li><li><a class="tocitem" href="../schur_complement/">Schur complement</a></li><li><a class="tocitem" href="../uniform_batch/">Uniform batch</a></li><li><a class="tocitem" href="../nonuniform_batch/">Non-uniform batch</a></li><li><a class="tocitem" href="../options/">Options</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Generic interface</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Generic interface</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/exanauts/CUDSS.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/exanauts/CUDSS.jl/blob/main/docs/src/generic.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h4 id="Generic-interface"><a class="docs-heading-anchor" href="#Generic-interface">Generic interface</a><a id="Generic-interface-1"></a><a class="docs-heading-anchor-permalink" href="#Generic-interface" title="Permalink"></a></h4><div class="admonition is-info" id="Note-35d7a787997e21e0"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-35d7a787997e21e0" title="Permalink"></a></header><div class="admonition-body"><p>The generic interface for uniform batches requires CUDSS.jl v0.6.4 and above.</p></div></div><h2 id="LLᵀ-and-LLᴴ"><a class="docs-heading-anchor" href="#LLᵀ-and-LLᴴ">LLᵀ and LLᴴ</a><a id="LLᵀ-and-LLᴴ-1"></a><a class="docs-heading-anchor-permalink" href="#LLᵀ-and-LLᴴ" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebra.cholesky-Union{Tuple{CuSparseMatrixCSR{T, INT}}, Tuple{INT}, Tuple{T}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><a class="docstring-binding" href="#LinearAlgebra.cholesky-Union{Tuple{CuSparseMatrixCSR{T, INT}}, Tuple{INT}, Tuple{T}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><code>LinearAlgebra.cholesky</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">solver = cholesky(A::CuSparseMatrixCSR{T,INT}; view::Char=&#39;F&#39;)</code></pre><p>Compute the LLᴴ factorization of a sparse matrix <code>A</code> on an NVIDIA GPU. The parameter type <code>T</code> is restricted to <code>Float32</code>, <code>Float64</code>, <code>ComplexF32</code>, or <code>ComplexF64</code>, while <code>INT</code> is restricted to <code>Int32</code> or <code>Int64</code>.</p><p><code>A</code> can also represent a batch of sparse matrices with a uniform sparsity pattern. The vectors <code>A.rowPtr</code> and <code>A.colVal</code> are identical to those of a single matrix, but the vector <code>A.nzVal</code> is a strided representation of the nonzeros of all matrices. We automatically detect whether we have a uniform batch or a single matrix by computing <code>length(A.nzVal) ÷ length(A.colVal)</code>.</p><p><strong>Input argument</strong></p><ul><li><code>A</code>: a sparse Hermitian positive definite matrix stored in the <code>CuSparseMatrixCSR</code> format.</li></ul><p><strong>Keyword argument</strong></p><p>*<code>view</code>: A character that specifies which triangle of the sparse matrix is provided. Possible options are <code>L</code> for the lower triangle, <code>U</code> for the upper triangle, and <code>F</code> for the full matrix.</p><p><strong>Output argument</strong></p><ul><li><code>solver</code>: Opaque structure <a href="../types/#CudssSolver"><code>CudssSolver</code></a> that stores the factors of the LLᴴ decomposition.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/exanauts/CUDSS.jl/blob/625679f81f2336d9c673e286c963bb8415a71808/src/generic.jl#L103-L124">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebra.cholesky!-Union{Tuple{INT}, Tuple{T}, Tuple{CudssSolver{T, INT}, CuSparseMatrixCSR{T, INT}}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><a class="docstring-binding" href="#LinearAlgebra.cholesky!-Union{Tuple{INT}, Tuple{T}, Tuple{CudssSolver{T, INT}, CuSparseMatrixCSR{T, INT}}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><code>LinearAlgebra.cholesky!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">solver = cholesky!(solver::CudssSolver{T,INT}, A::CuSparseMatrixCSR{T,INT})</code></pre><p>Compute the LLᴴ factorization of a sparse matrix <code>A</code> or a uniform batch of sparse matrices on an NVIDIA GPU, reusing the symbolic factorization stored in <code>solver</code>. The parameter type <code>T</code> is restricted to <code>Float32</code>, <code>Float64</code>, <code>ComplexF32</code>, or <code>ComplexF64</code>, while <code>INT</code> is restricted to <code>Int32</code> or <code>Int64</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/exanauts/CUDSS.jl/blob/625679f81f2336d9c673e286c963bb8415a71808/src/generic.jl#L141-L146">source</a></section></details></article><div class="admonition is-info" id="Note-fe820c999cd9969e"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-fe820c999cd9969e" title="Permalink"></a></header><div class="admonition-body"><p>If we only store one triangle of <code>A</code>, we can also use the wrappers <code>Symmetric</code> and <code>Hermitian</code> instead of using the keyword argument <code>view</code> in <code>cholesky</code>. For real matrices, both wrappers are allowed but only <code>Hermitian</code> can be used for complex matrices.</p></div></div><pre><code class="language-julia hljs">H = Hermitian(A, :U)
F = cholesky(H)</code></pre><pre><code class="language-julia hljs">using CUDA, CUDA.CUSPARSE
using CUDSS
using LinearAlgebra
using SparseArrays

T = ComplexF64
R = real(T)
n = 100
p = 5
A_cpu = sprand(T, n, n, 0.01)
A_cpu = A_cpu * A_cpu&#39; + I
B_cpu = rand(T, n, p)

A_gpu = CuSparseMatrixCSR(A_cpu |&gt; triu)
B_gpu = CuMatrix(B_cpu)
X_gpu = similar(B_gpu)

F = cholesky(A_gpu, view=&#39;U&#39;)
X_gpu = F \ B_gpu

R_gpu = B_gpu - CuSparseMatrixCSR(A_cpu) * X_gpu
norm(R_gpu)

# In-place LLᴴ
d_gpu = rand(R, n) |&gt; CuVector
A_gpu = A_gpu + Diagonal(d_gpu)
cholesky!(F, A_gpu)

C_cpu = rand(T, n, p)
C_gpu = CuMatrix(C_cpu)
ldiv!(X_gpu, F, C_gpu)

R_gpu = C_gpu - ( CuSparseMatrixCSR(A_cpu) + Diagonal(d_gpu) ) * X_gpu
norm(R_gpu)</code></pre><pre><code class="language-julia hljs">using CUDA, CUDA.CUSPARSE
using CUDSS
using LinearAlgebra
using SparseArrays

T = Float64
R = real(T)
n = 5
nbatch = 2
nnzA = 8
rowPtr = CuVector{Cint}([1, 3, 5, 7, 8, 9])
colVal = CuVector{Cint}([1, 3, 2, 3, 3, 5, 4, 5])
nzVal = CuVector{T}([4, 1, 3, 2, 5, 1, 1, 2,
                     2, 1, 3, 1, 6, 2, 4, 8])

As_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nzVal, (n,n));
F = cholesky(As_gpu; view=&#39;U&#39;)

bs_gpu = CuMatrix{T}([ 7 13;
                      12 15;
                      25 29;
                       4  8;
                      13 14])
xs_gpu = CuMatrix{T}(undef, n, nbatch)
ldiv!(xs_gpu, F, bs_gpu)

rs_gpu = Inf * ones(R, nbatch)
for i = 1:nbatch
    nz = nzVal[1 + (i-1) * nnzA : i * nnzA]
    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))
    A_cpu = SparseMatrixCSC(A_gpu)
    A_gpu = CuSparseMatrixCSR(A_cpu + A_cpu&#39; - Diagonal(A_cpu))
    b_gpu = bs_gpu[:, i]
    x_gpu = xs_gpu[:, i]
    r_gpu = b_gpu - A_gpu * x_gpu
    rs_gpu[i] = norm(r_gpu)
end
rs_gpu

new_nzVal = CuVector{T}([8, 2, 6, 4, 10, 2,  2,  4,
                         6, 3, 9, 3, 18, 6, 12, 24])
As_gpu.nzVal = new_nzVal
cholesky!(F, As_gpu)
ldiv!(xs_gpu, F, bs_gpu)

for i = 1:nbatch
    nz = new_nzVal[1 + (i-1) * nnzA : i * nnzA]
    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))
    A_cpu = SparseMatrixCSC(A_gpu)
    A_gpu = CuSparseMatrixCSR(A_cpu + A_cpu&#39; - Diagonal(A_cpu))
    b_gpu = bs_gpu[:, i]
    x_gpu = xs_gpu[:, i]
    r_gpu = b_gpu - A_gpu * x_gpu
    rs_gpu[i] = norm(r_gpu)
end
rs_gpu</code></pre><div class="admonition is-info" id="Note-3e94aa8351e7944c"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-3e94aa8351e7944c" title="Permalink"></a></header><div class="admonition-body"><p>When solving a uniform batch of linear systems, <code>b</code> and <code>x</code> can be vectors, matrices, or tensors. Internally, they are always treated as a single long vector in memory, with elements arranged consistently with Julia&#39;s column-major order across all dimensions. This vector contains all right-hand sides of the batch in a strided layout, and the dimensions of each right-hand side as well as the number of systems are tracked automatically, allowing <code>b</code> and <code>x</code> to be passed directly to <code>ldiv!</code> without manual reshaping.</p></div></div><h2 id="LDLᵀ-and-LDLᴴ"><a class="docs-heading-anchor" href="#LDLᵀ-and-LDLᴴ">LDLᵀ and LDLᴴ</a><a id="LDLᵀ-and-LDLᴴ-1"></a><a class="docs-heading-anchor-permalink" href="#LDLᵀ-and-LDLᴴ" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebra.ldlt-Union{Tuple{CuSparseMatrixCSR{T, INT}}, Tuple{INT}, Tuple{T}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><a class="docstring-binding" href="#LinearAlgebra.ldlt-Union{Tuple{CuSparseMatrixCSR{T, INT}}, Tuple{INT}, Tuple{T}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><code>LinearAlgebra.ldlt</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">solver = ldlt(A::CuSparseMatrixCSR{T,INT}; view::Char=&#39;F&#39;)</code></pre><p>Compute the LDLᴴ factorization of a sparse matrix <code>A</code> on an NVIDIA GPU. The parameter type <code>T</code> is restricted to <code>Float32</code>, <code>Float64</code>, <code>ComplexF32</code>, or <code>ComplexF64</code>, while <code>INT</code> is restricted to <code>Int32</code> or <code>Int64</code>.</p><p><code>A</code> can also represent a batch of sparse matrices with a uniform sparsity pattern. The vectors <code>A.rowPtr</code> and <code>A.colVal</code> are identical to those of a single matrix, but the vector <code>A.nzVal</code> is a strided representation of the nonzeros of all matrices. We automatically detect whether we have a uniform batch or a single matrix by computing <code>length(A.nzVal) ÷ length(A.colVal)</code>.</p><p><strong>Input argument</strong></p><ul><li><code>A</code>: a sparse Hermitian matrix stored in the <code>CuSparseMatrixCSR</code> format.</li></ul><p><strong>Keyword argument</strong></p><p>*<code>view</code>: A character that specifies which triangle of the sparse matrix is provided. Possible options are <code>L</code> for the lower triangle, <code>U</code> for the upper triangle, and <code>F</code> for the full matrix.</p><p><strong>Output argument</strong></p><ul><li><code>solver</code>: Opaque structure <a href="../types/#CudssSolver"><code>CudssSolver</code></a> that stores the factors of the LDLᴴ decomposition.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/exanauts/CUDSS.jl/blob/625679f81f2336d9c673e286c963bb8415a71808/src/generic.jl#L48-L69">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebra.ldlt!-Union{Tuple{INT}, Tuple{T}, Tuple{CudssSolver{T, INT}, CuSparseMatrixCSR{T, INT}}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><a class="docstring-binding" href="#LinearAlgebra.ldlt!-Union{Tuple{INT}, Tuple{T}, Tuple{CudssSolver{T, INT}, CuSparseMatrixCSR{T, INT}}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><code>LinearAlgebra.ldlt!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">solver = ldlt!(solver::CudssSolver{T,INT}, A::CuSparseMatrixCSR{T,INT})</code></pre><p>Compute the LDLᴴ factorization of a sparse matrix <code>A</code> or a uniform batch of sparse matrices on an NVIDIA GPU, reusing the symbolic factorization stored in <code>solver</code>. The parameter type <code>T</code> is restricted to <code>Float32</code>, <code>Float64</code>, <code>ComplexF32</code>, or <code>ComplexF64</code>, while <code>INT</code> is restricted to <code>Int32</code> or <code>Int64</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/exanauts/CUDSS.jl/blob/625679f81f2336d9c673e286c963bb8415a71808/src/generic.jl#L86-L91">source</a></section></details></article><div class="admonition is-info" id="Note-79c0ca7e45f19829"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-79c0ca7e45f19829" title="Permalink"></a></header><div class="admonition-body"><p>If we only store one triangle of <code>A_gpu</code>, we can also use the wrappers <code>Symmetric</code> and <code>Hermitian</code> instead of using the keyword argument <code>view</code> in <code>ldlt</code>. For real matrices, both wrappers are allowed but only <code>Hermitian</code> can be used for complex matrices.</p></div></div><pre><code class="language-julia hljs">S = Symmetric(A, :L)
F = ldlt(S)</code></pre><pre><code class="language-julia hljs">using CUDA, CUDA.CUSPARSE
using CUDSS
using LinearAlgebra
using SparseArrays

T = Float64
R = real(T)
n = 100
p = 5
A_cpu = sprand(T, n, n, 0.05) + I
A_cpu = A_cpu + A_cpu&#39;
B_cpu = rand(T, n, p)

A_gpu = CuSparseMatrixCSR(A_cpu |&gt; tril)
B_gpu = CuMatrix(B_cpu)
X_gpu = similar(B_gpu)

F = ldlt(A_gpu, view=&#39;L&#39;)
X_gpu = F \ B_gpu

R_gpu = B_gpu - CuSparseMatrixCSR(A_cpu) * X_gpu
norm(R_gpu)

# In-place LDLᵀ
d_gpu = rand(R, n) |&gt; CuVector
A_gpu = A_gpu + Diagonal(d_gpu)
ldlt!(F, A_gpu)

C_cpu = rand(T, n, p)
C_gpu = CuMatrix(C_cpu)
ldiv!(X_gpu, F, C_gpu)

R_gpu = C_gpu - ( CuSparseMatrixCSR(A_cpu) + Diagonal(d_gpu) ) * X_gpu
norm(R_gpu)</code></pre><pre><code class="language-julia hljs">using CUDA, CUDA.CUSPARSE
using CUDSS
using LinearAlgebra
using SparseArrays

T = ComplexF64
R = real(T)
n = 5
nbatch = 2
nrhs = 2
nnzA = 8
rowPtr = CuVector{Cint}([1, 2, 3, 6, 7, 9])
colVal = CuVector{Cint}([1, 2, 1, 2, 3, 4, 3, 5])
nzVal = CuVector{T}([4, 3, 1+im, 2-im, 5, 1, 1+im, 2,
                     2, 3, 1-im, 1+im, 6, 4, 2-im, 8])

As_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nzVal, (n,n));
F = ldlt(As_gpu; view=&#39;L&#39;)

Bs_gpu = CuVector{T}([ 7+im, 12+im, 25+im, 4+im, 13+im,  -7+im, -12+im, -25+im, -4+im, -13+im,
                      13-im, 15-im, 29-im, 8-im, 14-im, -13-im, -15-im, -29-im, -8-im, -14-im])
Bs_gpu = reshape(Bs_gpu, n, nrhs, nbatch)
Xs_gpu = CuArray{T}(undef, n, nrhs, nbatch)
ldiv!(Xs_gpu, F, Bs_gpu)

Rs_gpu = Inf * ones(R, nbatch)
for i = 1:nbatch
    nz = nzVal[1 + (i-1) * nnzA : i * nnzA]
    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))
    A_cpu = SparseMatrixCSC(A_gpu)
    A_gpu = CuSparseMatrixCSR(A_cpu + A_cpu&#39; - Diagonal(A_cpu))
    B_gpu = Bs_gpu[:, :, i]
    X_gpu = Xs_gpu[:, :, i]
    R_gpu = B_gpu - A_gpu * X_gpu
    Rs_gpu[i] = norm(R_gpu)
end
Rs_gpu

new_nzVal = CuVector{T}([-4, -3,  1-im, -2+im, -5, -1, -1-im, -2,
                         -2, -3, -1+im, -1-im, -6, -4, -2+im, -8])
As_gpu.nzVal = new_nzVal
ldlt!(F, As_gpu)

new_Bs_gpu = CuVector{T}([13-im, 15-im, 29-im, 8-im, 14-im, -13-im, -15-im, -29-im, -8-im, -14-im,
                          7+im, 12+im, 25+im, 4+im, 13+im,  -7+im, -12+im, -25+im, -4+im, -13+im])
new_Bs_gpu = reshape(new_Bs_gpu, n, nrhs, nbatch)
new_Xs_gpu = copy(new_Bs_gpu)
ldiv!(F, new_Xs_gpu)

for i = 1:nbatch
    nz = new_nzVal[1 + (i-1) * nnzA : i * nnzA]
    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))
    A_cpu = SparseMatrixCSC(A_gpu)
    A_gpu = CuSparseMatrixCSR(A_cpu + A_cpu&#39; - Diagonal(A_cpu))
    B_gpu = new_Bs_gpu[:, :, i]
    X_gpu = new_Xs_gpu[:, :, i]
    R_gpu = B_gpu - A_gpu * X_gpu
    Rs_gpu[i] = norm(R_gpu)
end
Rs_gpu</code></pre><h2 id="LU"><a class="docs-heading-anchor" href="#LU">LU</a><a id="LU-1"></a><a class="docs-heading-anchor-permalink" href="#LU" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebra.lu-Union{Tuple{CuSparseMatrixCSR{T, INT}}, Tuple{INT}, Tuple{T}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><a class="docstring-binding" href="#LinearAlgebra.lu-Union{Tuple{CuSparseMatrixCSR{T, INT}}, Tuple{INT}, Tuple{T}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><code>LinearAlgebra.lu</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">solver = lu(A::CuSparseMatrixCSR{T,INT})</code></pre><p>Compute the LU factorization of a sparse matrix <code>A</code> on an NVIDIA GPU. The parameter type <code>T</code> is restricted to <code>Float32</code>, <code>Float64</code>, <code>ComplexF32</code>, or <code>ComplexF64</code>, while <code>INT</code> is restricted to <code>Int32</code> or <code>Int64</code>.</p><p><code>A</code> can also represent a batch of sparse matrices with a uniform sparsity pattern. The vectors <code>A.rowPtr</code> and <code>A.colVal</code> are identical to those of a single matrix, but the vector <code>A.nzVal</code> is a strided representation of the nonzeros of all matrices. We automatically detect whether we have a uniform batch or a single matrix by computing <code>length(A.nzVal) ÷ length(A.colVal)</code>.</p><p><strong>Input argument</strong></p><ul><li><code>A</code>: a sparse square matrix stored in the <code>CuSparseMatrixCSR</code> format.</li></ul><p><strong>Output argument</strong></p><ul><li><code>solver</code>: an opaque structure <a href="../types/#CudssSolver"><code>CudssSolver</code></a> that stores the factors of the LU decomposition.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/exanauts/CUDSS.jl/blob/625679f81f2336d9c673e286c963bb8415a71808/src/generic.jl#L1-L18">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebra.lu!-Union{Tuple{INT}, Tuple{T}, Tuple{CudssSolver{T, INT}, CuSparseMatrixCSR{T, INT}}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><a class="docstring-binding" href="#LinearAlgebra.lu!-Union{Tuple{INT}, Tuple{T}, Tuple{CudssSolver{T, INT}, CuSparseMatrixCSR{T, INT}}} where {T&lt;:Union{Float32, Float64, ComplexF64, ComplexF32}, INT&lt;:Union{Int32, Int64}}"><code>LinearAlgebra.lu!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">solver = lu!(solver::CudssSolver{T,INT}, A::CuSparseMatrixCSR{T,INT})</code></pre><p>Compute the LU factorization of a sparse matrix <code>A</code> or a uniform batch of sparse matrices on an NVIDIA GPU, reusing the symbolic factorization stored in <code>solver</code>. The parameter type <code>T</code> is restricted to <code>Float32</code>, <code>Float64</code>, <code>ComplexF32</code>, or <code>ComplexF64</code>, while <code>INT</code> is restricted to <code>Int32</code> or <code>Int64</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/exanauts/CUDSS.jl/blob/625679f81f2336d9c673e286c963bb8415a71808/src/generic.jl#L31-L36">source</a></section></details></article><pre><code class="language-julia hljs">using CUDA, CUDA.CUSPARSE
using CUDSS
using LinearAlgebra
using SparseArrays

T = Float64
n = 100
A_cpu = sprand(T, n, n, 0.05) + I
b_cpu = rand(T, n)

A_gpu = CuSparseMatrixCSR(A_cpu)
b_gpu = CuVector(b_cpu)

F = lu(A_gpu)
x_gpu = F \ b_gpu

r_gpu = b_gpu - A_gpu * x_gpu
norm(r_gpu)

# In-place LU
d_gpu = rand(T, n) |&gt; CuVector
A_gpu = A_gpu + Diagonal(d_gpu)
lu!(F, A_gpu)

c_cpu = rand(T, n)
c_gpu = CuVector(c_cpu)
ldiv!(x_gpu, F, c_gpu)

r_gpu = c_gpu - A_gpu * x_gpu
norm(r_gpu)</code></pre><pre><code class="language-julia hljs">using CUDA, CUDA.CUSPARSE
using CUDSS
using LinearAlgebra
using SparseArrays

T = Float64
R = real(T)
n = 3
nbatch = 3

# Batch of unsymmetric linear systems
#        [1+λ  0   3  ]
# A(λ) = [ 4  5+λ  0  ]
#        [ 2   6  2+λ ]
nnzA = 7
rowPtr = CuVector{Cint}([1, 3, 5, 8])
colVal = CuVector{Cint}([1, 3, 1, 2, 1, 2, 3])

# List of values for λ
Λ = [1.0, 10.0, -20.0]
nzVal = CuVector{T}([1+Λ[1], 3, 4, 5+Λ[1], 2, 6, 2+Λ[1],
                     1+Λ[2], 3, 4, 5+Λ[2], 2, 6, 2+Λ[2],
                     1+Λ[3], 3, 4, 5+Λ[3], 2, 6, 2+Λ[3]])

Aλ_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nzVal, (n,n));
F = lu(Aλ_gpu)

bλ_gpu = CuMatrix{T}([1.0 4.0 7.0;
                      2.0 5.0 8.0;
                      3.0 6.0 9.0])
xλ_gpu = CuMatrix{T}(undef, n, nbatch)
ldiv!(xλ_gpu, F, bλ_gpu)

rλ_gpu = Inf * ones(R, nbatch)
for i = 1:nbatch
    nz = nzVal[1 + (i-1) * nnzA : i * nnzA]
    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))
    b_gpu = bλ_gpu[:, i]
    x_gpu = xλ_gpu[:, i]
    r_gpu = b_gpu - A_gpu * x_gpu
    rλ_gpu[i] = norm(r_gpu)
end
rλ_gpu

Λ = [-2.0, -10.0, 30.0]
new_nzVal = CuVector{T}([1+Λ[1], 3, 4, 5+Λ[1], 2, 6, 2+Λ[1],
                         1+Λ[2], 3, 4, 5+Λ[2], 2, 6, 2+Λ[2],
                         1+Λ[3], 3, 4, 5+Λ[3], 2, 6, 2+Λ[3]])
Aλ_gpu.nzVal = new_nzVal
lu!(F, Aλ_gpu)
ldiv!(xλ_gpu, F, bλ_gpu)

for i = 1:nbatch
    nz = new_nzVal[1 + (i-1) * nnzA : i * nnzA]
    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))
    b_gpu = bλ_gpu[:, i]
    x_gpu = xλ_gpu[:, i]
    r_gpu = b_gpu - A_gpu * x_gpu
    rλ_gpu[i] = norm(r_gpu)
end
rλ_gpu</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../cudss/">« cuDSS interface</a><a class="docs-footer-nextpage" href="../schur_complement/">Schur complement »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Sunday 21 December 2025 05:49">Sunday 21 December 2025</span>. Using Julia version 1.12.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
