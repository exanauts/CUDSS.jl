var documenterSearchIndex = {"docs":
[{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/#CudssSolver","page":"Types","title":"CudssSolver","text":"","category":"section"},{"location":"types/#CUDSS.CudssSolver","page":"Types","title":"CUDSS.CudssSolver","text":"solver = CudssSolver(A::CuSparseMatrixCSR{T,INT}, structure::String, view::Char; index::Char='O')\nsolver = CudssSolver(rowPtr::CuVector{INT}, colVal::CuVector{INT}, nzVal::CuVector{T}, structure::String, view::Char; index::Char='O')\nsolver = CudssSolver(matrix::CudssMatrix{T,INT}, config::CudssConfig, data::CudssData)\n\nThe parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\nCudssSolver contains all structures required to solve a linear system with cuDSS. It can also be used to solve a batch of linear systems sharing the same sparsity pattern. Two constructors of CudssSolver take as input the same parameters as CudssMatrix.\n\nstructure specifies the stucture for the sparse matrix:\n\n\"G\": General matrix – LDU factorization;\n\"S\": Real symmetric matrix – LDLᵀ factorization;\n\"H\": Complex Hermitian matrix – LDLᴴ factorization;\n\"SPD\": Symmetric positive-definite matrix – LLᵀ factorization;\n\"HPD\": Hermitian positive-definite matrix – LLᴴ factorization.\n\nview specifies matrix view for the sparse matrix:\n\n'L': Lower-triangular matrix and all values above the main diagonal are ignored;\n'U': Upper-triangular matrix and all values below the main diagonal are ignored;\n'F': Full matrix.\n\nindex specifies indexing base for the sparse matrix:\n\n'Z': 0-based indexing;\n'O': 1-based indexing.\n\nCudssSolver can be also constructed from the three structures CudssMatrix, CudssConfig and CudssData if needed.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssBatchedSolver","page":"Types","title":"CudssBatchedSolver","text":"","category":"section"},{"location":"types/#CUDSS.CudssBatchedSolver","page":"Types","title":"CUDSS.CudssBatchedSolver","text":"solver = CudssBatchedSolver(A::Vector{CuSparseMatrixCSR{T,INT}}, structure::String, view::Char; index::Char='O')\nsolver = CudssBatchedSolver(matrix::CudssBatchedMatrix{T,INT}, config::CudssConfig, data::CudssData)\n\nThe parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\nCudssBatchedSolver contains all structures required to solve a batch of linear systems with cuDSS. One constructor of CudssBatchedSolver takes as input the same parameters as CudssBatchedMatrix.\n\nstructure specifies the stucture for the sparse matrices:\n\n\"G\": General matrix – LDU factorization;\n\"S\": Real symmetric matrix – LDLᵀ factorization;\n\"H\": Complex Hermitian matrix – LDLᴴ factorization;\n\"SPD\": Symmetric positive-definite matrix – LLᵀ factorization;\n\"HPD\": Hermitian positive-definite matrix – LLᴴ factorization.\n\nview specifies matrix view for the sparse matrices:\n\n'L': Lower-triangular matrix and all values above the main diagonal are ignored;\n'U': Upper-triangular matrix and all values below the main diagonal are ignored;\n'F': Full matrix.\n\nindex specifies indexing base for the sparse matrices:\n\n'Z': 0-based indexing;\n'O': 1-based indexing.\n\nCudssBatchedSolver can be also constructed from the three structures CudssBatchedMatrix, CudssConfig and CudssData if needed.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssMatrix","page":"Types","title":"CudssMatrix","text":"","category":"section"},{"location":"types/#CUDSS.CudssMatrix","page":"Types","title":"CUDSS.CudssMatrix","text":"matrix = CudssMatrix(::Type{T}, n::Integer; nbatch::Integer=1)\nmatrix = CudssMatrix(::Type{T}, m::Integer, n::Integer; nbatch::Integer=1)\nmatrix = CudssMatrix(b::CuVector{T})\nmatrix = CudssMatrix(B::CuMatrix{T})\nmatrix = CudssMatrix(A::CuSparseMatrixCSR{T,INT}, struture::String, view::Char; index::Char='O')\nmatrix = CudssMatrix(rowPtr::CuVector{INT}, colVal::CuVector{INT}, nzVal::CuVector{T}, struture::String, view::Char; index::Char='O')\n\nThe parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\nCudssMatrix is a wrapper for CuVector, CuMatrix and CuSparseMatrixCSR. CudssMatrix can also represent a batch of CuSparseMatrixCSR sharing the same sparsity pattern. CudssMatrix is used to pass the matrix of the sparse linear system, as well as solution and right-hand side.\n\nstructure specifies the stucture for the sparse matrix:\n\n\"G\": General matrix – LDU factorization;\n\"S\": Real symmetric matrix – LDLᵀ factorization;\n\"H\": Complex Hermitian matrix – LDLᴴ factorization;\n\"SPD\": Symmetric positive-definite matrix – LLᵀ factorization;\n\"HPD\": Hermitian positive-definite matrix – LLᴴ factorization.\n\nview specifies matrix view for the sparse matrix:\n\n'L': Lower-triangular matrix and all values above the main diagonal are ignored;\n'U': Upper-triangular matrix and all values below the main diagonal are ignored;\n'F': Full matrix.\n\nindex specifies indexing base for the sparse matrix:\n\n'Z': 0-based indexing;\n'O': 1-based indexing.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssBatchedMatrix","page":"Types","title":"CudssBatchedMatrix","text":"","category":"section"},{"location":"types/#CUDSS.CudssBatchedMatrix","page":"Types","title":"CUDSS.CudssBatchedMatrix","text":"matrix = CudssBatchedMatrix(b::Vector{CuVector{T}})\nmatrix = CudssBatchedMatrix(B::Vector{CuMatrix{T}})\nmatrix = CudssBatchedMatrix(A::Vector{CuSparseMatrixCSR{T,INT}}, struture::String, view::Char; index::Char='O')\n\nThe parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\nCudssBatchedMatrix is a wrapper for Vector{CuVector}, Vector{CuMatrix} and Vector{CuSparseMatrixCSR}. CudssBatchedMatrix is used to pass the matrices of the sparse linear systems, as well as solutions and right-hand sides.\n\nstructure specifies the stucture for the sparse matrices:\n\n\"G\": General matrix – LDU factorization;\n\"S\": Real symmetric matrix – LDLᵀ factorization;\n\"H\": Complex Hermitian matrix – LDLᴴ factorization;\n\"SPD\": Symmetric positive-definite matrix – LLᵀ factorization;\n\"HPD\": Hermitian positive-definite matrix – LLᴴ factorization.\n\nview specifies matrix view for the sparse matrices:\n\n'L': Lower-triangular matrix and all values above the main diagonal are ignored;\n'U': Upper-triangular matrix and all values below the main diagonal are ignored;\n'F': Full matrix.\n\nindex specifies indexing base for the sparse matrices:\n\n'Z': 0-based indexing;\n'O': 1-based indexing.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssConfig","page":"Types","title":"CudssConfig","text":"","category":"section"},{"location":"types/#CUDSS.CudssConfig","page":"Types","title":"CUDSS.CudssConfig","text":"config = CudssConfig()\n\nCudssConfig stores configuration settings for the solver.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssData","page":"Types","title":"CudssData","text":"","category":"section"},{"location":"types/#CUDSS.CudssData","page":"Types","title":"CUDSS.CudssData","text":"data = CudssData()\ndata = CudssData(cudss_handle::cudssHandle_t)\n\nCudssData holds internal data (e.g., LU factors arrays).\n\n\n\n\n\n","category":"type"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/#cudss","page":"Functions","title":"cudss","text":"","category":"section"},{"location":"functions/#CUDSS.cudss","page":"Functions","title":"CUDSS.cudss","text":"cudss(phase::String, solver::CudssSolver{T}, x::CuVector{T}, b::CuVector{T})\ncudss(phase::String, solver::CudssSolver{T}, X::CuMatrix{T}, B::CuMatrix{T})\ncudss(phase::String, solver::CudssSolver{T}, X::CudssMatrix{T}, B::CudssMatrix{T})\ncudss(phase::String, solver::CudssBatchedSolver{T}, x::Vector{CuVector{T}}, b::Vector{CuVector{T}})\ncudss(phase::String, solver::CudssBatchedSolver{T}, X::Vector{CuMatrix{T}}, B::Vector{CuMatrix{T}})\ncudss(phase::String, solver::CudssBatchedSolver{T}, X::CudssBatchedMatrix{T}, B::CudssBatchedMatrix{T})\n\nThe parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64.\n\nThe available phases are:\n\n\"reordering\": Reordering;\n\"symbolic_factorization\": Symbolic factorization;\n\"analysis\": Reordering and symbolic factorization combined;\n\"factorization\": Numerical factorization;\n\"refactorization\": Numerical re-factorization;\n\"solve_fwd_perm\": Applying reordering permutation to the right hand side before the forward substitution;\n\"solve_fwd\": Forward substitution sub-step of the solving phase, including the local permutation due to partial pivoting;\n\"solve_diag\": Diagonal solve sub-step of the solving phase (only needed for symmetric / hermitian indefinite matrices);\n\"solve_bwd\": Backward substitution sub-step of the solving phase, including the local permutation due to partial pivoting;\n\"solve_bwd_perm\": Applying inverse reordering permutation to the intermediate solution after the backward substitution. If matching (and scaling) is enabled, this phase also includes applying the inverse matching permutation and inverse scaling (as the matching permutation and scalings were used to modify the matrix before the factorization);\n\"solve_refinement\": Iterative refinement;\n\"solve\": Full solving phase, combining all sub-phases and (optional) iterative refinement.\n\nWhen the Schur complement mode is enabled (option \"schur_mode\" set to 1), a specific combination of phases is required. For that reason, we added shorthand phases:\n\n\"solve_fwd_schur\": combines the phases \"solve_fwd_perm\", \"solve_fwd\", and \"solve_diag\";\n\"solve_bwd_schur\": combines the phases \"solve_bwd\" and \"solve_bwd_perm\".\n\n\n\n\n\n","category":"function"},{"location":"functions/#cudss_update","page":"Functions","title":"cudss_update","text":"","category":"section"},{"location":"functions/#CUDSS.cudss_update","page":"Functions","title":"CUDSS.cudss_update","text":"cudss_update(solver::CudssSolver{T,INT}, A::CuSparseMatrixCSR{T,INT})\ncudss_update(solver::CudssSolver{T,INT}, rowPtr::CuVector{INT}, colVal::CuVector{INT}, nzVal::CuVector{T})\ncudss_update(solver::CudssBatchedSolver{T,INT}, A::Vector{CuSparseMatrixCSR{T,INT}})\ncudss_update(matrix::CudssMatrix{T}, b::CuVector{T})\ncudss_update(matrix::CudssMatrix{T}, B::CuMatrix{T})\ncudss_update(matrix::CudssMatrix{T,INT}, A::CuSparseMatrixCSR{T,INT})\ncudss_update(matrix::CudssMatrix{T,INT}, rowPtr::CuVector{INT}, colVal::CuVector{INT}, nzVal::CuVector{T})\ncudss_update(matrix::CudssBatchedMatrix{T}, b::Vector{CuVector{T}})\ncudss_update(matrix::CudssBatchedMatrix{T}, B::Vector{CuMatrix{T}})\ncudss_update(matrix::CudssBatchedMatrix{T,INT}, A::Vector{CuSparseMatrixCSR{T,INT}})\n\nThe parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\nUpdate the contents of a CudssMatrix – CudssBatchedMatrix or CudssSolver – CudssBatchedSolver with new numerical values.\n\n\n\n\n\n","category":"function"},{"location":"functions/#cudss_set","page":"Functions","title":"cudss_set","text":"","category":"section"},{"location":"functions/#CUDSS.cudss_set","page":"Functions","title":"CUDSS.cudss_set","text":"cudss_set(solver::CudssSolver, parameter::String, value)\ncudss_set(solver::CudssBatchedSolver, parameter::String, value)\n\nThe available configuration parameters are:\n\n\"reordering_alg\": Algorithm for the reordering phase (\"default\", \"algo1\", \"algo2\", \"algo3\", \"algo4\", or \"algo5\");\n\"factorization_alg\": Algorithm for the factorization phase (\"default\", \"algo1\", \"algo2\", \"algo3\", \"algo4\", or \"algo5\");\n\"solve_alg\": Algorithm for the solving phase (\"default\", \"algo1\", \"algo2\", \"algo3\", \"algo4\", or \"algo5\");\n\"use_matching\": A flag to enable (1) or disable (0) the matching;\n\"matching_alg\": Algorithm for the matching;\n\"solve_mode\": Potential modificator on the system matrix (transpose or adjoint);\n\"ir_n_steps\": Number of steps during the iterative refinement;\n\"ir_tol\": Iterative refinement tolerance;\n\"pivot_type\": Type of pivoting ('C', 'R' or 'N');\n\"pivot_threshold\": Pivoting threshold which is used to determine if digonal element is subject to pivoting;\n\"pivot_epsilon\": Pivoting epsilon, absolute value to replace singular diagonal elements;\n\"max_lu_nnz\": Upper limit on the number of nonzero entries in LU factors for non-symmetric matrices;\n\"hybrid_memory_mode\": Hybrid memory mode – 0 (default = device-only) or 1 (hybrid = host/device);\n\"hybrid_device_memory_limit\": User-defined device memory limit (number of bytes) for the hybrid memory mode;\n\"use_cuda_register_memory\": A flag to enable (1) or disable (0) usage of cudaHostRegister() by the hybrid memory mode;\n\"host_nthreads\": Number of threads to be used by cuDSS in multi-threaded mode;\n\"hybrid_execute_mode\": Hybrid execute mode – 0 (default = device-only) or 1 (hybrid = host/device);\n\"pivot_epsilon_alg\": Algorithm for the pivot epsilon calculation;\n\"nd_nlevels\": Minimum number of levels for the nested dissection reordering;\n\"ubatch_size\": The number of matrices in a uniform batch of systems to be processed by cuDSS;\n\"ubatch_index\": Use -1 (default) to process all matrices in the uniform batch, or a 0-based index to process a single matrix during the factorization or solve phase;\n\"use_superpanels\": Use superpanel optimization – 1 (default = enabled) or 0 (disabled);\n\"device_count\": Device count in case of multiple device;\n\"device_indices\": A list of device indices as an integer array;\n\"schur_mode\": Schur complement mode – 0 (default = disabled) or 1 (enabled);\n\"deterministic_mode\": Enable deterministic mode – 0 (default = disabled) or 1 (enabled).\n\nThe available data parameters are:\n\n\"info\": Device-side error information;\n\"user_perm\": User permutation to be used instead of running the reordering algorithms;\n\"comm\": Communicator for Multi-GPU multi-node mode;\n\"user_elimination_tree\": User provided elimination tree information, which is used instead of running the reordering algorithm;\n\"user_schur_indices\": User-provided Schur complement indices. The provided buffer should be an integer array of size n, where n is the dimension of the matrix. The values should be equal to 1 for the rows / columns which are part of the Schur complement and 0 for the rest;\n\"user_host_interrupt\": User-provided host interrupt pointer;\n\"schur_matrix\": Schur complement matrix passed as a cudssMatrix_t object. It only updates the internal pointer for subsequent calls to cudss_get).\n\nThe data parameter \"info\" must be restored to 0 if a Cholesky factorization fails due to indefiniteness and refactorization is performed on an updated matrix.\n\n\n\n\n\n","category":"function"},{"location":"functions/#cudss_get","page":"Functions","title":"cudss_get","text":"","category":"section"},{"location":"functions/#CUDSS.cudss_get","page":"Functions","title":"CUDSS.cudss_get","text":"value = cudss_get(solver::CudssSolver, parameter::String)\nvalue = cudss_get(solver::CudssBatchedSolver, parameter::String)\n\nThe available configuration parameters are:\n\n\"reordering_alg\": Algorithm for the reordering phase;\n\"factorization_alg\": Algorithm for the factorization phase;\n\"solve_alg\": Algorithm for the solving phase;\n\"use_matching\": A flag to enable (1) or disable (0) the matching;\n\"matching_alg\": Algorithm for the matching;\n\"solve_mode\": Potential modificator on the system matrix (transpose or adjoint);\n\"ir_n_steps\": Number of steps during the iterative refinement;\n\"ir_tol\": Iterative refinement tolerance;\n\"pivot_type\": Type of pivoting;\n\"pivot_threshold\": Pivoting threshold which is used to determine if digonal element is subject to pivoting;\n\"pivot_epsilon\": Pivoting epsilon, absolute value to replace singular diagonal elements;\n\"max_lu_nnz\": Upper limit on the number of nonzero entries in LU factors for non-symmetric matrices;\n\"hybrid_memory_mode\": Hybrid memory mode – 0 (default = device-only) or 1 (hybrid = host/device);\n\"hybrid_device_memory_limit\": User-defined device memory limit (number of bytes) for the hybrid memory mode;\n\"use_cuda_register_memory\": A flag to enable (1) or disable (0) usage of cudaHostRegister() by the hybrid memory mode;\n\"host_nthreads\": Number of threads to be used by cuDSS in multi-threaded mode;\n\"hybrid_execute_mode\": Hybrid execute mode – 0 (default = device-only) or 1 (hybrid = host/device);\n\"pivot_epsilon_alg\": Algorithm for the pivot epsilon calculation;\n\"nd_nlevels\": Minimum number of levels for the nested dissection reordering;\n\"ubatch_size\": The number of matrices in a uniform batch of systems to be processed by cuDSS;\n\"ubatch_index\": Use -1 (default) to process all matrices in the uniform batch, or a 0-based index to process a single matrix during the factorization or solve phase;\n\"use_superpanels\": Use superpanel optimization – 1 (default = enabled) or 0 (disabled);\n\"device_count\": Device count in case of multiple device;\n\"device_indices\": A list of device indices as an integer array;\n\"schur_mode\": Schur complement mode – 0 (default = disabled) or 1 (enabled);\n\"deterministic_mode\": Enable deterministic mode – 0 (default = disabled) or 1 (enabled).\n\nThe available data parameters are:\n\n\"info\": Device-side error information;\n\"lu_nnz\": Number of non-zero entries in LU factors;\n\"npivots\": Number of pivots encountered during factorization;\n\"inertia\": Tuple of positive and negative indices of inertia for symmetric / hermitian indefinite matrices;\n\"perm_reorder_row\": Reordering permutation for the rows;\n\"perm_reorder_col\": Reordering permutation for the columns;\n\"perm_row\": Final row permutation (which includes effects of both reordering and pivoting);\n\"perm_col\": Final column permutation (which includes effects of both reordering and pivoting);\n\"perm_matching\": Matching (column) permutation Q such that A[:,Q] is reordered and then factorized;\n\"scale_row\": A vector of scaling factors applied to the rows of the factorized matrix;\n\"scale_col\": A vector of scaling factors applied to the columns of the factorized matrix;\n\"diag\": Diagonal of the factorized matrix;\n\"hybrid_device_memory_min\": Minimal amount of device memory (number of bytes) required in the hybrid memory mode;\n\"memory_estimates\": Memory estimates (in bytes) for host and device memory required for the chosen memory mode;\n\"nsuperpanels\": Number of superpanels in the matrix;\n\"schur_shape\": Shape of the Schur complement matrix as a triplet (nrows, ncols, nnz);\n\"schur_matrix\": Retrieve the Schur complement matrix;\n\"elimination_tree\": User provided elimination tree information, which is used instead of running the reordering algorithm. It must be used in combination with \"user_perm\" to have an effect.\n\nThe data parameters \"info\", \"lu_nnz\", \"perm_reorder_row\", \"perm_reorder_col\", \"perm_matching\", \"scale_row\", \"scale_col\", \"hybrid_device_memory_min\" and \"memory_estimates\" require the phase \"analyse\" performed by cudss. The data parameters \"npivots\", \"inertia\" and \"diag\" require the phases \"analyse\" and \"factorization\" performed by cudss. The data parameters \"perm_matching\", \"scale_row\", and \"scale_col\" require matching to be enabled (the configuration parameter \"use_matching\" must be set to 1).\n\n\n\n\n\n","category":"function"},{"location":"uniform_batch/#Batch-factorization-of-matrices-with-a-common-sparsity-pattern","page":"Uniform batch","title":"Batch factorization of matrices with a common sparsity pattern","text":"","category":"section"},{"location":"uniform_batch/","page":"Uniform batch","title":"Uniform batch","text":"note: Note\nThis functionality requires CUDSS.jl v0.5.3 and above.","category":"page"},{"location":"uniform_batch/#Batch-LU","page":"Uniform batch","title":"Batch LU","text":"","category":"section"},{"location":"uniform_batch/","page":"Uniform batch","title":"Uniform batch","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = Float64\nR = real(T)\nn = 3\nnbatch = 3\n\n# Collection of unsymmetric linear systems\n#        [1+λ  0   3  ]\n# A(λ) = [ 4  5+λ  0  ]\n#        [ 2   6  2+λ ]\nnnzA = 7\nrowPtr = CuVector{Cint}([1, 3, 5, 8])\ncolVal = CuVector{Cint}([1, 3, 1, 2, 1, 2, 3])\n\n# List of values for λ\nΛ = [1.0, 10.0, -20.0]\nnzVal = CuVector{T}([1+Λ[1], 3, 4, 5+Λ[1], 2, 6, 2+Λ[1],\n                     1+Λ[2], 3, 4, 5+Λ[2], 2, 6, 2+Λ[2],\n                     1+Λ[3], 3, 4, 5+Λ[3], 2, 6, 2+Λ[3]])\n\ncudss_bλ_gpu = CudssMatrix(T, n; nbatch)\nbλ_gpu = CuVector{T}([1.0, 2.0, 3.0,\n                      4.0, 5.0, 6.0,\n                      7.0, 8.0, 9.0])\ncudss_update(cudss_bλ_gpu, bλ_gpu)\n\ncudss_xλ_gpu = CudssMatrix(T, n; nbatch)\nxλ_gpu = CuVector{T}(undef, n * nbatch)\ncudss_update(cudss_xλ_gpu, xλ_gpu)\n\n# Constructor for uniform batch of systems\nsolver = CudssSolver(rowPtr, colVal, nzVal, \"G\", 'F')\n\n# Specify that it is a uniform batch of size \"nbatch\"\ncudss_set(solver, \"ubatch_size\", nbatch)\n\ncudss(\"analysis\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\ncudss(\"factorization\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\ncudss(\"solve\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\n\nrλ_gpu = rand(R, nbatch)\nfor i = 1:nbatch\n    nz = nzVal[1 + (i-1) * nnzA : i * nnzA]\n    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))\n    b_gpu = bλ_gpu[1 + (i-1) * n : i * n]\n    x_gpu = xλ_gpu[1 + (i-1) * n : i * n]\n    r_gpu = b_gpu - A_gpu * x_gpu\n    rλ_gpu[i] = norm(r_gpu)\nend\nrλ_gpu\n\n# Refactorize all matrices of the uniform batch\nΛ = [-2.0, -10.0, 30.0]\nnew_nzVal = CuVector{T}([1+Λ[1], 3, 4, 5+Λ[1], 2, 6, 2+Λ[1],\n                         1+Λ[2], 3, 4, 5+Λ[2], 2, 6, 2+Λ[2],\n                         1+Λ[3], 3, 4, 5+Λ[3], 2, 6, 2+Λ[3]])\n\ncudss_update(solver, rowPtr, colVal, new_nzVal)\ncudss(\"refactorization\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\ncudss(\"solve\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\n\nfor i = 1:nbatch\n    nz = new_nzVal[1 + (i-1) * nnzA : i * nnzA]\n    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))\n    b_gpu = bλ_gpu[1 + (i-1) * n : i * n]\n    x_gpu = xλ_gpu[1 + (i-1) * n : i * n]\n    r_gpu = b_gpu - A_gpu * x_gpu\n    rλ_gpu[i] = norm(r_gpu)\nend\nrλ_gpu","category":"page"},{"location":"uniform_batch/#Batch-LDLᵀ-and-LDLᴴ","page":"Uniform batch","title":"Batch LDLᵀ and LDLᴴ","text":"","category":"section"},{"location":"uniform_batch/","page":"Uniform batch","title":"Uniform batch","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = ComplexF64\nR = real(T)\nn = 5\nnbatch = 2\nnrhs = 2\n\nnnzA = 8\nrowPtr = CuVector{Cint}([1, 2, 3, 6, 7, 9])\ncolVal = CuVector{Cint}([1, 2, 1, 2, 3, 4, 3, 5])\nnzVal = CuVector{T}([4, 3, 1+im, 2-im, 5, 1, 1+im, 2,\n                     2, 3, 1-im, 1+im, 6, 4, 2-im, 8])\n\ncudss_Bs_gpu = CudssMatrix(T, n, nrhs; nbatch)\nBs_gpu = CuVector{T}([ 7+im, 12+im, 25+im, 4+im, 13+im,  -7+im, -12+im, -25+im, -4+im, -13+im,\n                      13-im, 15-im, 29-im, 8-im, 14-im, -13-im, -15-im, -29-im, -8-im, -14-im])\ncudss_update(cudss_Bs_gpu, Bs_gpu)\n\ncudss_Xs_gpu = CudssMatrix(T, n, nrhs; nbatch)\nXs_gpu = CuVector{T}(undef, n * nrhs * nbatch)\ncudss_update(cudss_Xs_gpu, Xs_gpu)\n\n# Constructor for uniform batch of systems\nsolver = CudssSolver(rowPtr, colVal, nzVal, \"H\", 'L')\n\n# Specify that it is a uniform batch of size \"nbatch\"\ncudss_set(solver, \"ubatch_size\", nbatch)\n\ncudss(\"analysis\", solver, cudss_Xs_gpu, cudss_Bs_gpu)\ncudss(\"factorization\", solver, cudss_Xs_gpu, cudss_Bs_gpu)\ncudss(\"solve\", solver, cudss_Xs_gpu, cudss_Bs_gpu)\n\nRs_gpu = rand(R, nbatch)\nfor i = 1:nbatch\n    nz = nzVal[1 + (i-1) * nnzA : i * nnzA]\n    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))\n    A_cpu = SparseMatrixCSC(A_gpu)\n    A_gpu = CuSparseMatrixCSR(A_cpu + A_cpu' - Diagonal(A_cpu))\n    B_gpu = reshape(Bs_gpu[1 + (i-1) * n * nrhs : i * n * nrhs], n, nrhs)\n    X_gpu = reshape(Xs_gpu[1 + (i-1) * n * nrhs : i * n * nrhs], n, nrhs)\n    R_gpu = B_gpu - A_gpu * X_gpu\n    Rs_gpu[i] = norm(R_gpu)\nend\nRs_gpu\n\nnew_nzVal = CuVector{T}([-4, -3,  1-im, -2+im, -5, -1, -1-im, -2,\n                         -2, -3, -1+im, -1-im, -6, -4, -2+im, -8])\ncudss_update(solver, rowPtr, colVal, new_nzVal)\ncudss(\"refactorization\", solver, cudss_Xs_gpu, cudss_Bs_gpu)\n\nnew_Bs_gpu = CuVector{T}([13-im, 15-im, 29-im, 8-im, 14-im, -13-im, -15-im, -29-im, -8-im, -14-im,\n                           7+im, 12+im, 25+im, 4+im, 13+im,  -7+im, -12+im, -25+im, -4+im, -13+im])\ncudss_update(cudss_Bs_gpu, new_Bs_gpu)\ncudss(\"solve\", solver, cudss_Xs_gpu, cudss_Bs_gpu)\n\nRs_gpu = rand(R, nbatch)\nfor i = 1:nbatch\n    nz = new_nzVal[1 + (i-1) * nnzA : i * nnzA]\n    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))\n    A_cpu = SparseMatrixCSC(A_gpu)\n    A_gpu = CuSparseMatrixCSR(A_cpu + A_cpu' - Diagonal(A_cpu))\n    B_gpu = reshape(new_Bs_gpu[1 + (i-1) * n * nrhs : i * n * nrhs], n, nrhs)\n    X_gpu = reshape(Xs_gpu[1 + (i-1) * n * nrhs : i * n * nrhs], n, nrhs)\n    R_gpu = B_gpu - A_gpu * X_gpu\n    Rs_gpu[i] = norm(R_gpu)\nend\nRs_gpu","category":"page"},{"location":"uniform_batch/#Batch-LLᵀ-and-LLᴴ","page":"Uniform batch","title":"Batch LLᵀ and LLᴴ","text":"","category":"section"},{"location":"uniform_batch/","page":"Uniform batch","title":"Uniform batch","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = Float64\nR = real(T)\nn = 5\nnbatch = 2\n\nnnzA = 8\nrowPtr = CuVector{Cint}([1, 3, 5, 7, 8, 9])\ncolVal = CuVector{Cint}([1, 3, 2, 3, 3, 5, 4, 5])\nnzVal = CuVector{T}([4, 1, 3, 2, 5, 1, 1, 2,\n                     2, 1, 3, 1, 6, 2, 4, 8])\n\ncudss_bs_gpu = CudssMatrix(T, n; nbatch)\nbs_gpu = CuVector{T}([ 7, 12, 25, 4, 13,\n                      13, 15, 29, 8, 14])\ncudss_update(cudss_bs_gpu, bs_gpu)\n\ncudss_xs_gpu = CudssMatrix(T, n; nbatch)\nxs_gpu = CuVector{T}(undef, n * nbatch)\ncudss_update(cudss_xs_gpu, xs_gpu)\n\n# Constructor for uniform batch of systems\nsolver = CudssSolver(rowPtr, colVal, nzVal, \"SPD\", 'U')\n\n# Specify that it is a uniform batch of size \"nbatch\"\ncudss_set(solver, \"ubatch_size\", nbatch)\n\ncudss(\"analysis\", solver, cudss_xs_gpu, cudss_bs_gpu)\ncudss(\"factorization\", solver, cudss_xs_gpu, cudss_bs_gpu)\ncudss(\"solve\", solver, cudss_xs_gpu, cudss_bs_gpu)\n\nrs_gpu = rand(R, nbatch)\nfor i = 1:nbatch\n    nz = nzVal[1 + (i-1) * nnzA : i * nnzA]\n    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))\n    A_cpu = SparseMatrixCSC(A_gpu)\n    A_gpu = CuSparseMatrixCSR(A_cpu + A_cpu' - Diagonal(A_cpu))\n    b_gpu = bs_gpu[1 + (i-1) * n : i * n]\n    x_gpu = xs_gpu[1 + (i-1) * n : i * n]\n    r_gpu = b_gpu - A_gpu * x_gpu\n    rs_gpu[i] = norm(r_gpu)\nend\nrs_gpu\n\nnew_nzVal = CuVector{T}([8, 2, 6, 4, 10, 2,  2,  4,\n                         6, 3, 9, 3, 18, 6, 12, 24])\ncudss_update(solver, rowPtr, colVal, new_nzVal)\ncudss(\"refactorization\", solver, cudss_xs_gpu, cudss_bs_gpu)\ncudss(\"solve\", solver, cudss_xs_gpu, cudss_bs_gpu)\n\nrs_gpu = rand(T, nbatch)\nfor i = 1:nbatch\n    nz = new_nzVal[1 + (i-1) * nnzA : i * nnzA]\n    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))\n    A_cpu = SparseMatrixCSC(A_gpu)\n    A_gpu = CuSparseMatrixCSR(A_cpu + A_cpu' - Diagonal(A_cpu))\n    b_gpu = bs_gpu[1 + (i-1) * n : i * n]\n    x_gpu = xs_gpu[1 + (i-1) * n : i * n]\n    r_gpu = b_gpu - A_gpu * x_gpu\n    rs_gpu[i] = norm(r_gpu)\nend\nrs_gpu","category":"page"},{"location":"nonuniform_batch/#Batch-factorization-of-matrices-with-different-sparsity-patterns","page":"Non-uniform batch","title":"Batch factorization of matrices with different sparsity patterns","text":"","category":"section"},{"location":"nonuniform_batch/","page":"Non-uniform batch","title":"Non-uniform batch","text":"note: Note\nThis functionality requires CUDSS.jl v0.4.2 and above.","category":"page"},{"location":"nonuniform_batch/#Batch-LU","page":"Non-uniform batch","title":"Batch LU","text":"","category":"section"},{"location":"nonuniform_batch/","page":"Non-uniform batch","title":"Non-uniform batch","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = Float64\nn = 100\nnbatch = 5\n\nbatch_A_gpu = CuSparseMatrixCSR{T,Cint}[]\nbatch_x_gpu = CuVector{T}[]\nbatch_b_gpu = CuVector{T}[]\n\nfor i = 1:nbatch\n    A_cpu = sprand(T, n, n, 0.05) + I\n    x_cpu = zeros(T, n)\n    b_cpu = rand(T, n)\n\n    push!(batch_A_gpu, A_cpu |> CuSparseMatrixCSR)\n    push!(batch_x_gpu, x_cpu |> CuVector)\n    push!(batch_b_gpu, b_cpu |> CuVector)\nend\n\nsolver = CudssBatchedSolver(batch_A_gpu, \"G\", 'F')\n\ncudss(\"analysis\", solver, batch_x_gpu, batch_b_gpu)\ncudss(\"factorization\", solver, batch_x_gpu, batch_b_gpu)\ncudss(\"solve\", solver, batch_x_gpu, batch_b_gpu)\n\nbatch_r_gpu = batch_b_gpu .- batch_A_gpu .* batch_x_gpu\nnorm.(batch_r_gpu)\n\n# In-place LU\nfor i = 1:nbatch\n    d_gpu = rand(T, n) |> CuVector\n    batch_A_gpu[i] = batch_A_gpu[i] + Diagonal(d_gpu)\nend\ncudss_update(solver, batch_A_gpu)\n\nfor i = 1:nbatch\n    c_cpu = rand(T, n)\n    c_gpu = CuVector(c_cpu)\n    batch_b_gpu[i] = c_gpu\nend\n\ncudss(\"refactorization\", solver, batch_x_gpu, batch_b_gpu)\ncudss(\"solve\", solver, batch_x_gpu, batch_b_gpu)\n\nbatch_r_gpu = batch_b_gpu .- batch_A_gpu .* batch_x_gpu\nnorm.(batch_r_gpu)","category":"page"},{"location":"nonuniform_batch/#Batch-LDLᵀ-and-LDLᴴ","page":"Non-uniform batch","title":"Batch LDLᵀ and LDLᴴ","text":"","category":"section"},{"location":"nonuniform_batch/","page":"Non-uniform batch","title":"Non-uniform batch","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = Float64\nR = real(T)\nn = 100\np = 5\nnbatch = 10\n\nbatch_A_cpu = SparseMatrixCSC{T}[]\nbatch_A_gpu = CuSparseMatrixCSR{T,Cint}[]\nbatch_X_gpu = CuMatrix{T}[]\nbatch_B_gpu = CuMatrix{T}[]\n\nfor i = 1:nbatch\n    A_cpu = sprand(T, n, n, 0.05) + I\n    A_cpu = A_cpu + A_cpu'\n    X_cpu = zeros(T, n, p)\n    B_cpu = rand(T, n, p)\n\n    push!(batch_A_cpu, A_cpu)\n    push!(batch_A_gpu, A_cpu |> tril |> CuSparseMatrixCSR)\n    push!(batch_X_gpu, X_cpu |> CuMatrix)\n    push!(batch_B_gpu, B_cpu |> CuMatrix)\nend\n\nstructure = T <: Real ? \"S\" : \"H\"\nsolver = CudssBatchedSolver(batch_A_gpu, structure, 'L')\n\ncudss(\"analysis\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"factorization\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"solve\", solver, batch_X_gpu, batch_B_gpu)\n\nbatch_R_gpu = batch_B_gpu .- CuSparseMatrixCSR.(batch_A_cpu) .* batch_X_gpu\nnorm.(batch_R_gpu)\n\n# In-place LDLᵀ\nd_cpu = rand(R, n)\nd_gpu = CuVector(d_cpu)\nfor i = 1:nbatch\n    batch_A_gpu[i] = batch_A_gpu[i] + Diagonal(d_gpu)\n    batch_A_cpu[i] = batch_A_cpu[i] + Diagonal(d_cpu)\nend\ncudss_update(solver, batch_A_gpu)\n\nfor i = 1:nbatch\n    C_cpu = rand(T, n, p)\n    C_gpu = CuMatrix(C_cpu)\n    batch_B_gpu[i] = C_gpu\nend\n\ncudss(\"refactorization\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"solve\", solver, batch_X_gpu, batch_B_gpu)\n\nbatch_R_gpu = batch_B_gpu .- CuSparseMatrixCSR.(batch_A_cpu) .* batch_X_gpu\nnorm.(batch_R_gpu)","category":"page"},{"location":"nonuniform_batch/#Batch-LLᵀ-and-LLᴴ","page":"Non-uniform batch","title":"Batch LLᵀ and LLᴴ","text":"","category":"section"},{"location":"nonuniform_batch/","page":"Non-uniform batch","title":"Non-uniform batch","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = ComplexF64\nR = real(T)\nn = 100\np = 4\nnbatch = 8\n\nbatch_A_cpu = SparseMatrixCSC{T}[]\nbatch_A_gpu = CuSparseMatrixCSR{T,Cint}[]\nbatch_X_gpu = CuMatrix{T}[]\nbatch_B_gpu = CuMatrix{T}[]\n\nfor i = 1:nbatch\n    A_cpu = sprand(T, n, n, 0.05) + I\n    A_cpu = A_cpu * A_cpu' + I\n    X_cpu = zeros(T, n, p)\n    B_cpu = rand(T, n, p)\n\n    push!(batch_A_cpu, A_cpu)\n    push!(batch_A_gpu, A_cpu |> triu |> CuSparseMatrixCSR)\n    push!(batch_X_gpu, X_cpu |> CuMatrix)\n    push!(batch_B_gpu, B_cpu |> CuMatrix)\nend\n\nstructure = T <: Real ? \"SPD\" : \"HPD\"\nsolver = CudssBatchedSolver(batch_A_gpu, structure, 'U')\n\ncudss(\"analysis\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"factorization\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"solve\", solver, batch_X_gpu, batch_B_gpu)\n\nbatch_R_gpu = batch_B_gpu .- CuSparseMatrixCSR.(batch_A_cpu) .* batch_X_gpu\nnorm.(batch_R_gpu)\n\n# In-place LLᴴ\nd_cpu = rand(R, n)\nd_gpu = CuVector(d_cpu)\nfor i = 1:nbatch\n    batch_A_gpu[i] = batch_A_gpu[i] + Diagonal(d_gpu)\n    batch_A_cpu[i] = batch_A_cpu[i] + Diagonal(d_cpu)\nend\ncudss_update(solver, batch_A_gpu)\n\nfor i = 1:nbatch\n    C_cpu = rand(T, n, p)\n    C_gpu = CuMatrix(C_cpu)\n    batch_B_gpu[i] = C_gpu\nend\n\ncudss(\"refactorization\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"solve\", solver, batch_X_gpu, batch_B_gpu)\n\nbatch_R_gpu = batch_B_gpu .- CuSparseMatrixCSR.(batch_A_cpu) .* batch_X_gpu\nnorm.(batch_R_gpu)","category":"page"},{"location":"options/#Iterative-refinement","page":"Options","title":"Iterative refinement","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = Float64\nn = 100\np = 5\nA_cpu = sprand(T, n, n, 0.01)\nA_cpu = A_cpu + I\nB_cpu = rand(T, n, p)\n\nA_gpu = CuSparseMatrixCSR(A_cpu)\nB_gpu = CuMatrix(B_cpu)\nX_gpu = similar(B_gpu)\n\nsolver = CudssSolver(A_gpu, \"G\", 'F')\n\n# Perform one step of iterative refinement\nir = 1\ncudss_set(solver, \"ir_n_steps\", ir)\n\ncudss(\"analysis\", solver, X_gpu, B_gpu)\ncudss(\"factorization\", solver, X_gpu, B_gpu)\ncudss(\"solve\", solver, X_gpu, B_gpu)\n\nR_gpu = B_gpu - CuSparseMatrixCSR(A_cpu) * X_gpu\nnorm(R_gpu)","category":"page"},{"location":"options/#User-permutation","page":"Options","title":"User permutation","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\nusing AMD\n\nT = ComplexF64\nn = 100\nA_cpu = sprand(T, n, n, 0.01)\nA_cpu = A_cpu' * A_cpu + I\nb_cpu = rand(T, n)\n\nA_gpu = CuSparseMatrixCSR(A_cpu)\nb_gpu = CuVector(b_cpu)\nx_gpu = similar(b_gpu)\n\nsolver = CudssSolver(A_gpu, \"HPD\", 'F')\n\n# Provide a user permutation\npermutation = amd(A_cpu) |> Vector{Cint}\ncudss_set(solver, \"user_perm\", permutation)\n\ncudss(\"analysis\", solver, x_gpu, b_gpu)\ncudss(\"factorization\", solver, x_gpu, b_gpu)\ncudss(\"solve\", solver, x_gpu, b_gpu)\n\nr_gpu = b_gpu - CuSparseMatrixCSR(A_cpu) * x_gpu\nnorm(r_gpu)","category":"page"},{"location":"options/#Hybrid-memory-mode","page":"Options","title":"Hybrid memory mode","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = Float64\nn = 100\nA_cpu = sprand(T, n, n, 0.01)\nA_cpu = A_cpu + A_cpu' + I\nb_cpu = rand(T, n)\n\nA_gpu = CuSparseMatrixCSR(A_cpu)\nb_gpu = CuVector(b_cpu)\nx_gpu = similar(b_gpu)\n\nsolver = CudssSolver(A_gpu, \"S\", 'F')\n\n# Use the hybrid mode (host and device memory)\ncudss_set(solver, \"hybrid_mode\", 1)\n\ncudss(\"analysis\", solver, x_gpu, b_gpu)\n\n# Minimal amount of device memory required in the hybrid memory mode.\nnbytes_gpu = cudss_get(solver, \"hybrid_device_memory_min\")\n\n# Device memory limit for the hybrid memory mode.\n# Only use it if you don't want to rely on the internal default heuristic.\ncudss_set(solver, \"hybrid_device_memory_limit\", nbytes_gpu)\n\ncudss(\"factorization\", solver, x_gpu, b_gpu)\ncudss(\"solve\", solver, x_gpu, b_gpu)\n\nr_gpu = b_gpu - CuSparseMatrixCSR(A_cpu) * x_gpu\nnorm(r_gpu)","category":"page"},{"location":"options/#Selecting-matrices-in-a-uniform-batch","page":"Options","title":"Selecting matrices in a uniform batch","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = Float64\nn = 3\nnbatch = 3\n\n# Collection of unsymmetric linear systems\n#        [1+λ  0   3  ]\n# A(λ) = [ 4  5+λ  0  ]\n#        [ 2   6  2+λ ]\nnnzA = 7\nrowPtr = CuVector{Cint}([1, 3, 5, 8])\ncolVal = CuVector{Cint}([1, 3, 1, 2, 1, 2, 3])\n\n# List of values for λ\nΛ = [1.0, 10.0, -20.0]\nnzVal = CuVector{T}([1+Λ[1], 3, 4, 5+Λ[1], 2, 6, 2+Λ[1],\n                     1+Λ[2], 3, 4, 5+Λ[2], 2, 6, 2+Λ[2],\n                     1+Λ[3], 3, 4, 5+Λ[3], 2, 6, 2+Λ[3]])\n\nbλ_gpu = CuVector{T}([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])\nxλ_gpu = CuVector{T}(undef, n * nbatch)\ncudss_bλ_gpu = CudssMatrix(T, n; nbatch)\ncudss_xλ_gpu = CudssMatrix(T, n; nbatch)\ncudss_update(cudss_bλ_gpu, bλ_gpu)\ncudss_update(cudss_xλ_gpu, xλ_gpu)\n\n# Constructor for uniform batch of systems\nsolver = CudssSolver(rowPtr, colVal, nzVal, \"G\", 'F')\n\n# Specify that it is a uniform batch of size \"nbatch\"\ncudss_set(solver, \"ubatch_size\", nbatch)\n\ncudss(\"analysis\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\ncudss(\"factorization\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\ncudss(\"solve\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\n\nrλ_gpu = rand(T, nbatch)\nfor i = 1:nbatch\n    nz = nzVal[1 + (i-1) * nnzA : i * nnzA]\n    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))\n    b_gpu = bλ_gpu[1 + (i-1) * n : i * n]\n    x_gpu = xλ_gpu[1 + (i-1) * n : i * n]\n    r_gpu = b_gpu - A_gpu * x_gpu\n    rλ_gpu[i] = norm(r_gpu)\nend\nrλ_gpu\n\n# Solve only the first linear system with the new right-hand sides\ncudss_set(solver, \"ubatch_index\", 0)  # 0-based index of the first matrix\n\ncλ_gpu = CuVector{T}([10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0])\ncudss_update(cudss_bλ_gpu, cλ_gpu)\ncudss(\"solve\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\n\nfor i = 1:nbatch\n    nz = nzVal[1 + (i-1) * nnzA : i * nnzA]\n    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))\n    c_gpu = cλ_gpu[1 + (i-1) * n : i * n]\n    x_gpu = xλ_gpu[1 + (i-1) * n : i * n]\n    r_gpu = c_gpu - A_gpu * x_gpu\n    rλ_gpu[i] = norm(r_gpu)\nend\nrλ_gpu\n\n# Refactorize only the first matrix of the batch\nΛ = [-2.0, -10.0, 30.0]\nnew_nzVal = CuVector{T}([1+Λ[1], 3, 4, 5+Λ[1], 2, 6, 2+Λ[1],\n                         1+Λ[2], 3, 4, 5+Λ[2], 2, 6, 2+Λ[2],\n                         1+Λ[3], 3, 4, 5+Λ[3], 2, 6, 2+Λ[3]])\ncudss_update(solver, rowPtr, colVal, new_nzVal)\ncudss(\"refactorization\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\ncudss(\"solve\", solver, cudss_xλ_gpu, cudss_bλ_gpu)\n\nfor i = 1:nbatch\n    nz = new_nzVal[1 + (i-1) * nnzA : i * nnzA]\n    A_gpu = CuSparseMatrixCSR{T,Cint}(rowPtr, colVal, nz, (n,n))\n    c_gpu = cλ_gpu[1 + (i-1) * n : i * n]\n    x_gpu = xλ_gpu[1 + (i-1) * n : i * n]\n    r_gpu = c_gpu - A_gpu * x_gpu\n    rλ_gpu[i] = norm(r_gpu)\nend\nrλ_gpu\n\n# Process again all matrices at once in the uniform batch\ncudss_set(solver, \"ubatch_index\", -1)","category":"page"},{"location":"options/#Schur-complement","page":"Options","title":"Schur complement","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = Float64\nn = 5\n\n# A = [A₁₁ A₁₂] where A₁₁ = [4 0], A₁₂ = [1 0 2]\n#     [A₂₁ A₂₂]             [0 5]        [0 3 0]\n#\n# A₂₁ = [0 6] and A₂₂ = [8 0 0 ]\n#       [7 0]           [0 9 1 ]\n#       [0 0]           [0 1 10]\n#\n# The matrix A and the Schur complement S = A₂₂ − A₂₁(A₁₁)⁻¹A₁₂ are:\n#\n#     [4 0 1 0 2 ]\n#     [0 5 0 3 0 ]          [ 8    -3.6  0  ]\n# A = [0 6 8 0 0 ]  and S = [-1.75  9   -2.5]\n#     [7 0 0 9 1 ]          [ 0     1    10 ]\n#     [0 0 0 1 10]\n\nrows = [1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 5, 5]\ncols = [1, 3, 5, 2, 4, 2, 3, 1, 4, 5, 4, 5]\nvals = [4.0, 1.0, 2.0, 5.0, 3.0, 6.0, 8.0, 7.0, 9.0, 1.0, 1.0, 10.0]\nA_cpu = sparse(rows, cols, vals, n, n)\n\n# Right-hand side such the solution is a vector of ones\nb_cpu = [7.0, 8.0, 14.0, 17.0, 11.0]\n\nA_gpu = CuSparseMatrixCSR(A_cpu)\nx_gpu = CuVector{Float64}(undef, n)\nb_gpu = CuVector(b_cpu)\nsolver = CudssSolver(A_gpu, \"G\", 'F')\n\n# Enable the Schur complement computation\ncudss_set(solver, \"schur_mode\", 1)\n\n# Rows and columns for the Schur complement of the block A₂₂\nschur_indices = Cint[0, 0, 1, 1, 1]\ncudss_set(solver, \"user_schur_indices\", schur_indices)\n\n# Compute the Schur complement\ncudss(\"analysis\", solver, x_gpu, b_gpu)\ncudss(\"factorization\", solver, x_gpu, b_gpu)\n\n# Dimension of the Schur complement nₛ and the number of nonzeros\n(nrows_S, ncols_S, nnz_S) = cudss_get(solver, \"schur_shape\")\n\n# Storage for the Schur complement\nS_gpu = CuMatrix{Float64}(undef, nrows_S, ncols_S)\ncudss_matrix = CudssMatrix(S_gpu)\n\n# Update the content of S_gpu\ncudss_set(solver, \"schur_matrix\", cudss_matrix.matrix)\ncudss_get(solver, \"schur_matrix\")\n\n# [A₁₁ A₁₂] [x₁] = [b₁] ⟺ A₁₁x₁ = b₁ - A₁₂x₂\n# [A₂₁ A₂₂] [x₂]   [b₂]   Sx₂ = b₂ - A₂₁(A₁₁)⁻¹b₁ = bₛ\n\n# Compute bₛ with a partial forward solve\n# bₛ is stored in the last nₛ components of b_gpu\ncudss(\"solve_fwd_schur\", solver, x_gpu, b_gpu)\n\n# Compute x₂ with the dense LU of cuSOLVER\nnₛ = 3\nbₛ = b_gpu[n-nₛ+1:n]\nx₂ = S \\ bₛ\n\n# Compute x₁ with a partial backward solve\n# x₂ must be store the last nₛ components of x_gpu\nx_gpu[n-nₛ+1:n] .= x2\ncudss(\"solve_bwd_schur\", solver, x_gpu, b_gpu)","category":"page"},{"location":"#Home","page":"Home","title":"CUDSS.jl documentation","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CUDSS.jl is a Julia interface to the NVIDIA cuDSS library. NVIDIA cuDSS provides three factorizations (LDU, LDLᵀ, LLᵀ) for solving sparse linear systems on GPUs. For more details on using cuDSS, refer to the official cuDSS documentation.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> ]\npkg> add CUDSS\npkg> test CUDSS","category":"page"},{"location":"generic/#Generic-interface","page":"Generic interface","title":"Generic interface","text":"","category":"section"},{"location":"generic/#LLᵀ-and-LLᴴ","page":"Generic interface","title":"LLᵀ and LLᴴ","text":"","category":"section"},{"location":"generic/#LinearAlgebra.cholesky-Union{Tuple{CuSparseMatrixCSR{T, INT}}, Tuple{INT}, Tuple{T}} where {T<:Union{Float32, Float64, ComplexF64, ComplexF32}, INT<:Union{Int32, Int64}}","page":"Generic interface","title":"LinearAlgebra.cholesky","text":"solver = cholesky(A::CuSparseMatrixCSR{T,INT}; view::Char='F')\n\nCompute the LLᴴ factorization of a sparse matrix A on an NVIDIA GPU. The parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\nInput argument\n\nA: a sparse Hermitian positive definite matrix stored in the CuSparseMatrixCSR format.\n\nKeyword argument\n\n*view: A character that specifies which triangle of the sparse matrix is provided. Possible options are L for the lower triangle, U for the upper triangle, and F for the full matrix.\n\nOutput argument\n\nsolver: Opaque structure CudssSolver that stores the factors of the LLᴴ decomposition.\n\n\n\n\n\n","category":"method"},{"location":"generic/#LinearAlgebra.cholesky!-Union{Tuple{INT}, Tuple{T}, Tuple{CudssSolver{T, INT}, CuSparseMatrixCSR{T, INT}}} where {T<:Union{Float32, Float64, ComplexF64, ComplexF32}, INT<:Union{Int32, Int64}}","page":"Generic interface","title":"LinearAlgebra.cholesky!","text":"solver = cholesky!(solver::CudssSolver{T,INT}, A::CuSparseMatrixCSR{T,INT})\n\nCompute the LLᴴ factorization of a sparse matrix A on an NVIDIA GPU, reusing the symbolic factorization stored in solver. The parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\n\n\n\n\n","category":"method"},{"location":"generic/","page":"Generic interface","title":"Generic interface","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = ComplexF64\nR = real(T)\nn = 100\np = 5\nA_cpu = sprand(T, n, n, 0.01)\nA_cpu = A_cpu * A_cpu' + I\nB_cpu = rand(T, n, p)\n\nA_gpu = CuSparseMatrixCSR(A_cpu |> triu)\nB_gpu = CuMatrix(B_cpu)\nX_gpu = similar(B_gpu)\n\nF = cholesky(A_gpu, view='U')\nX_gpu = F \\ B_gpu\n\nR_gpu = B_gpu - CuSparseMatrixCSR(A_cpu) * X_gpu\nnorm(R_gpu)\n\n# In-place LLᴴ\nd_gpu = rand(R, n) |> CuVector\nA_gpu = A_gpu + Diagonal(d_gpu)\ncholesky!(F, A_gpu)\n\nC_cpu = rand(T, n, p)\nC_gpu = CuMatrix(C_cpu)\nldiv!(X_gpu, F, C_gpu)\n\nR_gpu = C_gpu - ( CuSparseMatrixCSR(A_cpu) + Diagonal(d_gpu) ) * X_gpu\nnorm(R_gpu)","category":"page"},{"location":"generic/","page":"Generic interface","title":"Generic interface","text":"note: Note\nIf we only store one triangle of A_gpu, we can also use the wrappers Symmetric and Hermitian instead of using the keyword argument view in cholesky. For real matrices, both wrappers are allowed but only Hermitian can be used for complex matrices.","category":"page"},{"location":"generic/","page":"Generic interface","title":"Generic interface","text":"H_gpu = Hermitian(A_gpu, :U)\nF = cholesky(H_gpu)","category":"page"},{"location":"generic/#LDLᵀ-and-LDLᴴ","page":"Generic interface","title":"LDLᵀ and LDLᴴ","text":"","category":"section"},{"location":"generic/#LinearAlgebra.ldlt-Union{Tuple{CuSparseMatrixCSR{T, INT}}, Tuple{INT}, Tuple{T}} where {T<:Union{Float32, Float64, ComplexF64, ComplexF32}, INT<:Union{Int32, Int64}}","page":"Generic interface","title":"LinearAlgebra.ldlt","text":"solver = ldlt(A::CuSparseMatrixCSR{T,INT}; view::Char='F')\n\nCompute the LDLᴴ factorization of a sparse matrix A on an NVIDIA GPU. The parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\nInput argument\n\nA: a sparse Hermitian matrix stored in the CuSparseMatrixCSR format.\n\nKeyword argument\n\n*view: A character that specifies which triangle of the sparse matrix is provided. Possible options are L for the lower triangle, U for the upper triangle, and F for the full matrix.\n\nOutput argument\n\nsolver: Opaque structure CudssSolver that stores the factors of the LDLᴴ decomposition.\n\n\n\n\n\n","category":"method"},{"location":"generic/#LinearAlgebra.ldlt!-Union{Tuple{INT}, Tuple{T}, Tuple{CudssSolver{T, INT}, CuSparseMatrixCSR{T, INT}}} where {T<:Union{Float32, Float64, ComplexF64, ComplexF32}, INT<:Union{Int32, Int64}}","page":"Generic interface","title":"LinearAlgebra.ldlt!","text":"solver = ldlt!(solver::CudssSolver{T,INT}, A::CuSparseMatrixCSR{T,INT})\n\nCompute the LDLᴴ factorization of a sparse matrix A on an NVIDIA GPU, reusing the symbolic factorization stored in solver. The parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\n\n\n\n\n","category":"method"},{"location":"generic/","page":"Generic interface","title":"Generic interface","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = Float64\nR = real(T)\nn = 100\np = 5\nA_cpu = sprand(T, n, n, 0.05) + I\nA_cpu = A_cpu + A_cpu'\nB_cpu = rand(T, n, p)\n\nA_gpu = CuSparseMatrixCSR(A_cpu |> tril)\nB_gpu = CuMatrix(B_cpu)\nX_gpu = similar(B_gpu)\n\nF = ldlt(A_gpu, view='L')\nX_gpu = F \\ B_gpu\n\nR_gpu = B_gpu - CuSparseMatrixCSR(A_cpu) * X_gpu\nnorm(R_gpu)\n\n# In-place LDLᵀ\nd_gpu = rand(R, n) |> CuVector\nA_gpu = A_gpu + Diagonal(d_gpu)\nldlt!(F, A_gpu)\n\nC_cpu = rand(T, n, p)\nC_gpu = CuMatrix(C_cpu)\nldiv!(X_gpu, F, C_gpu)\n\nR_gpu = C_gpu - ( CuSparseMatrixCSR(A_cpu) + Diagonal(d_gpu) ) * X_gpu\nnorm(R_gpu)","category":"page"},{"location":"generic/","page":"Generic interface","title":"Generic interface","text":"note: Note\nIf we only store one triangle of A_gpu, we can also use the wrappers Symmetric and Hermitian instead of using the keyword argument view in ldlt. For real matrices, both wrappers are allowed but only Hermitian can be used for complex matrices.","category":"page"},{"location":"generic/","page":"Generic interface","title":"Generic interface","text":"S_gpu = Symmetric(A_gpu, :L)\nF = ldlt(S_gpu)","category":"page"},{"location":"generic/#LU","page":"Generic interface","title":"LU","text":"","category":"section"},{"location":"generic/#LinearAlgebra.lu-Union{Tuple{CuSparseMatrixCSR{T, INT}}, Tuple{INT}, Tuple{T}} where {T<:Union{Float32, Float64, ComplexF64, ComplexF32}, INT<:Union{Int32, Int64}}","page":"Generic interface","title":"LinearAlgebra.lu","text":"solver = lu(A::CuSparseMatrixCSR{T,INT})\n\nCompute the LU factorization of a sparse matrix A on an NVIDIA GPU. The parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\nInput argument\n\nA: a sparse square matrix stored in the CuSparseMatrixCSR format.\n\nOutput argument\n\nsolver: an opaque structure CudssSolver that stores the factors of the LU decomposition.\n\n\n\n\n\n","category":"method"},{"location":"generic/#LinearAlgebra.lu!-Union{Tuple{INT}, Tuple{T}, Tuple{CudssSolver{T, INT}, CuSparseMatrixCSR{T, INT}}} where {T<:Union{Float32, Float64, ComplexF64, ComplexF32}, INT<:Union{Int32, Int64}}","page":"Generic interface","title":"LinearAlgebra.lu!","text":"solver = lu!(solver::CudssSolver{T,INT}, A::CuSparseMatrixCSR{T,INT})\n\nCompute the LU factorization of a sparse matrix A on an NVIDIA GPU, reusing the symbolic factorization stored in solver. The parameter type T is restricted to Float32, Float64, ComplexF32, or ComplexF64, while INT is restricted to Int32 or Int64.\n\n\n\n\n\n","category":"method"},{"location":"generic/","page":"Generic interface","title":"Generic interface","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = Float64\nn = 100\nA_cpu = sprand(T, n, n, 0.05) + I\nb_cpu = rand(T, n)\n\nA_gpu = CuSparseMatrixCSR(A_cpu)\nb_gpu = CuVector(b_cpu)\n\nF = lu(A_gpu)\nx_gpu = F \\ b_gpu\n\nr_gpu = b_gpu - A_gpu * x_gpu\nnorm(r_gpu)\n\n# In-place LU\nd_gpu = rand(T, n) |> CuVector\nA_gpu = A_gpu + Diagonal(d_gpu)\nlu!(F, A_gpu)\n\nc_cpu = rand(T, n)\nc_gpu = CuVector(c_cpu)\nldiv!(x_gpu, F, c_gpu)\n\nr_gpu = c_gpu - A_gpu * x_gpu\nnorm(r_gpu)","category":"page"}]
}
