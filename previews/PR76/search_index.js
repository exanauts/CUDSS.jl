var documenterSearchIndex = {"docs":
[{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/#CudssSolver","page":"Types","title":"CudssSolver","text":"","category":"section"},{"location":"types/#CUDSS.CudssSolver","page":"Types","title":"CUDSS.CudssSolver","text":"solver = CudssSolver(A::CuSparseMatrixCSR{T,Cint}, structure::String, view::Char; index::Char='O')\nsolver = CudssSolver(matrix::CudssMatrix{T}, config::CudssConfig, data::CudssData)\n\nThe type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\nCudssSolver contains all structures required to solve a linear system with cuDSS. One constructor of CudssSolver takes as input the same parameters as CudssMatrix.\n\nstructure specifies the stucture for the sparse matrix:\n\n\"G\": General matrix – LDU factorization;\n\"S\": Real symmetric matrix – LDLᵀ factorization;\n\"H\": Complex Hermitian matrix – LDLᴴ factorization;\n\"SPD\": Symmetric positive-definite matrix – LLᵀ factorization;\n\"HPD\": Hermitian positive-definite matrix – LLᴴ factorization.\n\nview specifies matrix view for the sparse matrix:\n\n'L': Lower-triangular matrix and all values above the main diagonal are ignored;\n'U': Upper-triangular matrix and all values below the main diagonal are ignored;\n'F': Full matrix.\n\nindex specifies indexing base for the sparse matrix:\n\n'Z': 0-based indexing;\n'O': 1-based indexing.\n\nCudssSolver can be also constructed from the three structures CudssMatrix, CudssConfig and CudssData if needed.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssBatchedSolver","page":"Types","title":"CudssBatchedSolver","text":"","category":"section"},{"location":"types/#CUDSS.CudssBatchedSolver","page":"Types","title":"CUDSS.CudssBatchedSolver","text":"solver = CudssBatchedSolver(A::CuSparseMatrixCSR{T,Cint}, structure::String, view::Char; index::Char='O')\nsolver = CudssBatchedSolver(matrix::CudssBatchedMatrix{T}, config::CudssConfig, data::CudssData)\n\nThe type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\nCudssBatchedSolver contains all structures required to solve a batch of linear systems with cuDSS. One constructor of CudssBatchedSolver takes as input the same parameters as CudssBatchedMatrix.\n\nstructure specifies the stucture for the sparse matrices:\n\n\"G\": General matrix – LDU factorization;\n\"S\": Real symmetric matrix – LDLᵀ factorization;\n\"H\": Complex Hermitian matrix – LDLᴴ factorization;\n\"SPD\": Symmetric positive-definite matrix – LLᵀ factorization;\n\"HPD\": Hermitian positive-definite matrix – LLᴴ factorization.\n\nview specifies matrix view for the sparse matrices:\n\n'L': Lower-triangular matrix and all values above the main diagonal are ignored;\n'U': Upper-triangular matrix and all values below the main diagonal are ignored;\n'F': Full matrix.\n\nindex specifies indexing base for the sparse matrices:\n\n'Z': 0-based indexing;\n'O': 1-based indexing.\n\nCudssBatchedSolver can be also constructed from the three structures CudssBatchedMatrix, CudssConfig and CudssData if needed.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssMatrix","page":"Types","title":"CudssMatrix","text":"","category":"section"},{"location":"types/#CUDSS.CudssMatrix","page":"Types","title":"CUDSS.CudssMatrix","text":"matrix = CudssMatrix(b::CuVector{T})\nmatrix = CudssMatrix(B::CuMatrix{T})\nmatrix = CudssMatrix(A::CuSparseMatrixCSR{T,Cint}, struture::String, view::Char; index::Char='O')\n\nThe type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\nCudssMatrix is a wrapper for CuVector, CuMatrix and CuSparseMatrixCSR. CudssMatrix is used to pass the matrix of the sparse linear system, as well as solution and right-hand side.\n\nstructure specifies the stucture for the sparse matrix:\n\n\"G\": General matrix – LDU factorization;\n\"S\": Real symmetric matrix – LDLᵀ factorization;\n\"H\": Complex Hermitian matrix – LDLᴴ factorization;\n\"SPD\": Symmetric positive-definite matrix – LLᵀ factorization;\n\"HPD\": Hermitian positive-definite matrix – LLᴴ factorization.\n\nview specifies matrix view for the sparse matrix:\n\n'L': Lower-triangular matrix and all values above the main diagonal are ignored;\n'U': Upper-triangular matrix and all values below the main diagonal are ignored;\n'F': Full matrix.\n\nindex specifies indexing base for the sparse matrix:\n\n'Z': 0-based indexing;\n'O': 1-based indexing.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssBatchedMatrix","page":"Types","title":"CudssBatchedMatrix","text":"","category":"section"},{"location":"types/#CUDSS.CudssBatchedMatrix","page":"Types","title":"CUDSS.CudssBatchedMatrix","text":"matrix = CudssBatchedMatrix(b::Vector{CuVector{T}})\nmatrix = CudssBatchedMatrix(B::Vector{CuMatrix{T}})\nmatrix = CudssBatchedMatrix(A::Vector{CuSparseMatrixCSR{T,Cint}}, struture::String, view::Char; index::Char='O')\n\nThe type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\nCudssBatchedMatrix is a wrapper for Vector{CuVector}, Vector{CuMatrix} and Vector{CuSparseMatrixCSR}. CudssBatchedMatrix is used to pass the matrices of the sparse linear systems, as well as solutions and right-hand sides.\n\nstructure specifies the stucture for the sparse matrices:\n\n\"G\": General matrix – LDU factorization;\n\"S\": Real symmetric matrix – LDLᵀ factorization;\n\"H\": Complex Hermitian matrix – LDLᴴ factorization;\n\"SPD\": Symmetric positive-definite matrix – LLᵀ factorization;\n\"HPD\": Hermitian positive-definite matrix – LLᴴ factorization.\n\nview specifies matrix view for the sparse matrices:\n\n'L': Lower-triangular matrix and all values above the main diagonal are ignored;\n'U': Upper-triangular matrix and all values below the main diagonal are ignored;\n'F': Full matrix.\n\nindex specifies indexing base for the sparse matrices:\n\n'Z': 0-based indexing;\n'O': 1-based indexing.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssConfig","page":"Types","title":"CudssConfig","text":"","category":"section"},{"location":"types/#CUDSS.CudssConfig","page":"Types","title":"CUDSS.CudssConfig","text":"config = CudssConfig()\n\nCudssConfig stores configuration settings for the solver.\n\n\n\n\n\n","category":"type"},{"location":"types/#CudssData","page":"Types","title":"CudssData","text":"","category":"section"},{"location":"types/#CUDSS.CudssData","page":"Types","title":"CUDSS.CudssData","text":"data = CudssData()\ndata = CudssData(cudss_handle::cudssHandle_t)\n\nCudssData holds internal data (e.g., LU factors arrays).\n\n\n\n\n\n","category":"type"},{"location":"batch/#Batch-LU","page":"Batch API","title":"Batch LU","text":"","category":"section"},{"location":"batch/","page":"Batch API","title":"Batch API","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = Float64\nn = 100\nnbatch = 5\n\nbatch_A_gpu = CuSparseMatrixCSR{T,Cint}[]\nbatch_x_gpu = CuVector{T}[]\nbatch_b_gpu = CuVector{T}[]\n\nfor i = 1:nbatch\n    A_cpu = sprand(T, n, n, 0.05) + I\n    x_cpu = zeros(T, n)\n    b_cpu = rand(T, n)\n\n    push!(batch_A_gpu, A_cpu |> CuSparseMatrixCSR)\n    push!(batch_x_gpu, x_cpu |> CuVector)\n    push!(batch_b_gpu, b_cpu |> CuVector)\nend\n\nsolver = CudssBatchedSolver(batch_A_gpu, \"G\", 'F')\n\ncudss(\"analysis\", solver, batch_x_gpu, batch_b_gpu)\ncudss(\"factorization\", solver, batch_x_gpu, batch_b_gpu)\ncudss(\"solve\", solver, batch_x_gpu, batch_b_gpu)\n\nbatch_r_gpu = batch_b_gpu .- batch_A_gpu .* batch_x_gpu\nnorm.(batch_r_gpu)\n\n# In-place LU\nfor i = 1:nbatch\n    d_gpu = rand(T, n) |> CuVector\n    batch_A_gpu[i] = batch_A_gpu[i] + Diagonal(d_gpu)\nend\ncudss_set(solver, batch_A_gpu)\n\nfor i = 1:nbatch\n    c_cpu = rand(T, n)\n    c_gpu = CuVector(c_cpu)\n    batch_b_gpu[i] = c_gpu\nend\n\ncudss(\"refactorization\", solver, batch_x_gpu, batch_b_gpu)\ncudss(\"solve\", solver, batch_x_gpu, batch_b_gpu)\n\nbatch_r_gpu = batch_b_gpu .- batch_A_gpu .* batch_x_gpu\nnorm.(batch_r_gpu)","category":"page"},{"location":"batch/#Batch-LDLᵀ-and-LDLᴴ","page":"Batch API","title":"Batch LDLᵀ and LDLᴴ","text":"","category":"section"},{"location":"batch/","page":"Batch API","title":"Batch API","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = Float64\nR = real(T)\nn = 100\np = 5\nnbatch = 10\n\nbatch_A_cpu = SparseMatrixCSC{T}[]\nbatch_A_gpu = CuSparseMatrixCSR{T,Cint}[]\nbatch_X_gpu = CuMatrix{T}[]\nbatch_B_gpu = CuMatrix{T}[]\n\nfor i = 1:nbatch\n    A_cpu = sprand(T, n, n, 0.05) + I\n    A_cpu = A_cpu + A_cpu'\n    X_cpu = zeros(T, n, p)\n    B_cpu = rand(T, n, p)\n\n    push!(batch_A_cpu, A_cpu)\n    push!(batch_A_gpu, A_cpu |> tril |> CuSparseMatrixCSR)\n    push!(batch_X_gpu, X_cpu |> CuMatrix)\n    push!(batch_B_gpu, B_cpu |> CuMatrix)\nend\n\nstructure = T <: Real ? \"S\" : \"H\"\nsolver = CudssBatchedSolver(batch_A_gpu, structure, 'L')\n\ncudss(\"analysis\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"factorization\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"solve\", solver, batch_X_gpu, batch_B_gpu)\n\nbatch_R_gpu = batch_B_gpu .- CuSparseMatrixCSR.(batch_A_cpu) .* batch_X_gpu\nnorm.(batch_R_gpu)\n\n# In-place LDLᵀ\nd_cpu = rand(R, n)\nd_gpu = CuVector(d_cpu)\nfor i = 1:nbatch\n    batch_A_gpu[i] = batch_A_gpu[i] + Diagonal(d_gpu)\n    batch_A_cpu[i] = batch_A_cpu[i] + Diagonal(d_cpu)\nend\ncudss_set(solver, batch_A_gpu)\n\nfor i = 1:nbatch\n    C_cpu = rand(T, n, p)\n    C_gpu = CuMatrix(C_cpu)\n    batch_B_gpu[i] = C_gpu\nend\n\ncudss(\"refactorization\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"solve\", solver, batch_X_gpu, batch_B_gpu)\n\nbatch_R_gpu = batch_B_gpu .- CuSparseMatrixCSR.(batch_A_cpu) .* batch_X_gpu\nnorm.(batch_R_gpu)","category":"page"},{"location":"batch/#Batch-LLᵀ-and-LLᴴ","page":"Batch API","title":"Batch LLᵀ and LLᴴ","text":"","category":"section"},{"location":"batch/","page":"Batch API","title":"Batch API","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing SparseArrays, LinearAlgebra\n\nT = ComplexF64\nR = real(T)\nn = 100\np = 4\nnbatch = 8\n\nbatch_A_cpu = SparseMatrixCSC{T}[]\nbatch_A_gpu = CuSparseMatrixCSR{T,Cint}[]\nbatch_X_gpu = CuMatrix{T}[]\nbatch_B_gpu = CuMatrix{T}[]\n\nfor i = 1:nbatch\n    A_cpu = sprand(T, n, n, 0.05) + I\n    A_cpu = A_cpu * A_cpu' + I\n    X_cpu = zeros(T, n, p)\n    B_cpu = rand(T, n, p)\n\n    push!(batch_A_cpu, A_cpu)\n    push!(batch_A_gpu, A_cpu |> triu |> CuSparseMatrixCSR)\n    push!(batch_X_gpu, X_cpu |> CuMatrix)\n    push!(batch_B_gpu, B_cpu |> CuMatrix)\nend\n\nstructure = T <: Real ? \"SPD\" : \"HPD\"\nsolver = CudssBatchedSolver(batch_A_gpu, structure, 'U')\n\ncudss(\"analysis\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"factorization\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"solve\", solver, batch_X_gpu, batch_B_gpu)\n\nbatch_R_gpu = batch_B_gpu .- CuSparseMatrixCSR.(batch_A_cpu) .* batch_X_gpu\nnorm.(batch_R_gpu)\n\n# In-place LLᴴ\nd_cpu = rand(R, n)\nd_gpu = CuVector(d_cpu)\nfor i = 1:nbatch\n    batch_A_gpu[i] = batch_A_gpu[i] + Diagonal(d_gpu)\n    batch_A_cpu[i] = batch_A_cpu[i] + Diagonal(d_cpu)\nend\ncudss_set(solver, batch_A_gpu)\n\nfor i = 1:nbatch\n    C_cpu = rand(T, n, p)\n    C_gpu = CuMatrix(C_cpu)\n    batch_B_gpu[i] = C_gpu\nend\n\ncudss(\"refactorization\", solver, batch_X_gpu, batch_B_gpu)\ncudss(\"solve\", solver, batch_X_gpu, batch_B_gpu)\n\nbatch_R_gpu = batch_B_gpu .- CuSparseMatrixCSR.(batch_A_cpu) .* batch_X_gpu\nnorm.(batch_R_gpu)","category":"page"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/#cudss","page":"Functions","title":"cudss","text":"","category":"section"},{"location":"functions/#CUDSS.cudss","page":"Functions","title":"CUDSS.cudss","text":"cudss(phase::String, solver::CudssSolver{T}, x::CuVector{T}, b::CuVector{T})\ncudss(phase::String, solver::CudssSolver{T}, X::CuMatrix{T}, B::CuMatrix{T})\ncudss(phase::String, solver::CudssSolver{T}, X::CudssMatrix{T}, B::CudssMatrix{T})\ncudss(phase::String, solver::CudssBatchedSolver{T}, x::Vector{CuVector{T}}, b::Vector{CuVector{T}})\ncudss(phase::String, solver::CudssBatchedSolver{T}, X::Vector{CuMatrix{T}}, B::Vector{CuMatrix{T}})\ncudss(phase::String, solver::CudssBatchedSolver{T}, X::CudssBatchedMatrix{T}, B::CudssBatchedMatrix{T})\n\nThe type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\nThe available phases are \"reordering\", \"symbolic_factorization\", \"analysis\", \"factorization\", \"refactorization\" and \"solve\". The phases \"solve_fwd\", \"solve_diag\" and \"solve_bwd\" are available but not yet functional.\n\n\n\n\n\n","category":"function"},{"location":"functions/#cudss_set","page":"Functions","title":"cudss_set","text":"","category":"section"},{"location":"functions/#CUDSS.cudss_set","page":"Functions","title":"CUDSS.cudss_set","text":"cudss_set(solver::CudssSolver, parameter::String, value)\ncudss_set(solver::CudssSolver{T}, A::CuSparseMatrixCSR{T,Cint})\ncudss_set(solver::CudssBatchedSolver, parameter::String, value)\ncudss_set(solver::CudssBatchedSolver{T}, A::Vector{CuSparseMatrixCSR{T,Cint}})\ncudss_set(config::CudssConfig, parameter::String, value)\ncudss_set(data::CudssData, parameter::String, value)\ncudss_set(matrix::CudssMatrix{T}, b::CuVector{T})\ncudss_set(matrix::CudssMatrix{T}, B::CuMatrix{T})\ncudss_set(matrix::CudssMatrix{T}, A::CuSparseMatrixCSR{T,Cint})\ncudss_set(matrix::CudssBatchedMatrix{T}, b::Vector{CuVector{T}})\ncudss_set(matrix::CudssBatchedMatrix{T}, B::Vector{CuMatrix{T}})\ncudss_set(matrix::CudssBatchedMatrix{T}, A::Vector{CuSparseMatrixCSR{T,Cint}})\n\nThe type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\nThe available configuration parameters are:\n\n\"reordering_alg\": Algorithm for the reordering phase (\"default\", \"algo1\", \"algo2\", \"algo3\", \"algo4\", or \"algo5\");\n\"factorization_alg\": Algorithm for the factorization phase (\"default\", \"algo1\", \"algo2\", \"algo3\", \"algo4\", or \"algo5\");\n\"solve_alg\": Algorithm for the solving phase (\"default\", \"algo1\", \"algo2\", \"algo3\", \"algo4\", or \"algo5\");\n\"use_matching\": A flag to enable (1) or disable (0) the matching;\n\"matching_alg\": Algorithm for the matching;\n\"solve_mode\": Potential modificator on the system matrix (transpose or adjoint);\n\"ir_n_steps\": Number of steps during the iterative refinement;\n\"ir_tol\": Iterative refinement tolerance;\n\"pivot_type\": Type of pivoting ('C', 'R' or 'N');\n\"pivot_threshold\": Pivoting threshold which is used to determine if digonal element is subject to pivoting;\n\"pivot_epsilon\": Pivoting epsilon, absolute value to replace singular diagonal elements;\n\"max_lu_nnz\": Upper limit on the number of nonzero entries in LU factors for non-symmetric matrices;\n\"hybrid_mode\": Memory mode – 0 (default = device-only) or 1 (hybrid = host/device);\n\"hybrid_device_memory_limit\": User-defined device memory limit (number of bytes) for the hybrid memory mode;\n\"use_cuda_register_memory\": A flag to enable (1) or disable (0) usage of cudaHostRegister() by the hybrid memory mode;\n\"host_nthreads\": Number of threads to be used by cuDSS in multi-threaded mode;\n\"hybrid_execute_mode\": Hybrid execute mode – 0 (default = device-only) or 1 (hybrid = host/device);\n\"pivot_epsilon_alg\": Algorithm for the pivot epsilon calculation;\n\"nd_nlevels\": Minimum number of levels for the nested dissection reordering;\n\"ubatch_size\": The number of matrices in a uniform batch of systems to be processed by cuDSS;\n\"ubatch_index\": Specify cuDSS to process all matrices in the uniform batch at once.\n\nThe available data parameters are:\n\n\"info\": Device-side error information;\n\"user_perm\": User permutation to be used instead of running the reordering algorithms;\n\"comm\": Communicator for Multi-GPU multi-node mode.\n\nThe data parameter \"info\" must be restored to 0 if a Cholesky factorization fails due to indefiniteness and refactorization is performed on an updated matrix.\n\n\n\n\n\n","category":"function"},{"location":"functions/#cudss_get","page":"Functions","title":"cudss_get","text":"","category":"section"},{"location":"functions/#CUDSS.cudss_get","page":"Functions","title":"CUDSS.cudss_get","text":"value = cudss_get(solver::CudssSolver, parameter::String)\nvalue = cudss_get(solver::CudssBatchedSolver, parameter::String)\nvalue = cudss_get(config::CudssConfig, parameter::String)\nvalue = cudss_get(data::CudssData, parameter::String)\n\nThe available configuration parameters are:\n\n\"reordering_alg\": Algorithm for the reordering phase;\n\"factorization_alg\": Algorithm for the factorization phase;\n\"solve_alg\": Algorithm for the solving phase;\n\"use_matching\": A flag to enable (1) or disable (0) the matching;\n\"matching_alg\": Algorithm for the matching;\n\"solve_mode\": Potential modificator on the system matrix (transpose or adjoint);\n\"ir_n_steps\": Number of steps during the iterative refinement;\n\"ir_tol\": Iterative refinement tolerance;\n\"pivot_type\": Type of pivoting;\n\"pivot_threshold\": Pivoting threshold which is used to determine if digonal element is subject to pivoting;\n\"pivot_epsilon\": Pivoting epsilon, absolute value to replace singular diagonal elements;\n\"max_lu_nnz\": Upper limit on the number of nonzero entries in LU factors for non-symmetric matrices;\n\"hybrid_mode\": Memory mode – 0 (default = device-only) or 1 (hybrid = host/device);\n\"hybrid_device_memory_limit\": User-defined device memory limit (number of bytes) for the hybrid memory mode;\n\"use_cuda_register_memory\": A flag to enable (1) or disable (0) usage of cudaHostRegister() by the hybrid memory mode;\n\"host_nthreads\": Number of threads to be used by cuDSS in multi-threaded mode;\n\"hybrid_execute_mode\": Hybrid execute mode – 0 (default = device-only) or 1 (hybrid = host/device);\n\"pivot_epsilon_alg\": Algorithm for the pivot epsilon calculation;\n\"nd_nlevels\": Minimum number of levels for the nested dissection reordering;\n\"ubatch_size\": The number of matrices in a uniform batch of systems to be processed by cuDSS;\n\"ubatch_index\": Specify cuDSS to process all matrices in the uniform batch at once.\n\nThe available data parameters are:\n\n\"info\": Device-side error information;\n\"lu_nnz\": Number of non-zero entries in LU factors;\n\"npivots\": Number of pivots encountered during factorization;\n\"inertia\": Tuple of positive and negative indices of inertia for symmetric and hermitian non positive-definite matrix types;\n\"perm_reorder_row\": Reordering permutation for the rows;\n\"perm_reorder_col\": Reordering permutation for the columns;\n\"perm_row\": Final row permutation (which includes effects of both reordering and pivoting);\n\"perm_col\": Final column permutation (which includes effects of both reordering and pivoting);\n\"diag\": Diagonal of the factorized matrix;\n\"hybrid_device_memory_min\": Minimal amount of device memory (number of bytes) required in the hybrid memory mode;\n\"memory_estimates\": Memory estimates (in bytes) for host and device memory required for the chosen memory mode.\n\nThe data parameters \"info\", \"lu_nnz\", \"perm_reorder_row\", \"perm_reorder_col\", \"hybrid_device_memory_min\" and \"memory_estimates\" require the phase \"analyse\" performed by cudss. The data parameters \"npivots\", \"inertia\" and \"diag\" require the phases \"analyse\" and \"factorization\" performed by cudss. The data parameters \"perm_row\" and \"perm_col\" are available but not yet functional.\n\n\n\n\n\n","category":"function"},{"location":"options/#Iterative-refinement","page":"Options","title":"Iterative refinement","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = Float64\nn = 100\np = 5\nA_cpu = sprand(T, n, n, 0.01)\nA_cpu = A_cpu + I\nB_cpu = rand(T, n, p)\n\nA_gpu = CuSparseMatrixCSR(A_cpu)\nB_gpu = CuMatrix(B_cpu)\nX_gpu = similar(B_gpu)\n\nsolver = CudssSolver(A_gpu, \"G\", 'F')\n\n# Perform one step of iterative refinement\nir = 1\ncudss_set(solver, \"ir_n_steps\", ir)\n\ncudss(\"analysis\", solver, X_gpu, B_gpu)\ncudss(\"factorization\", solver, X_gpu, B_gpu)\ncudss(\"solve\", solver, X_gpu, B_gpu)\n\nR_gpu = B_gpu - CuSparseMatrixCSR(A_cpu) * X_gpu\nnorm(R_gpu)","category":"page"},{"location":"options/#User-permutation","page":"Options","title":"User permutation","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\nusing AMD\n\nT = ComplexF64\nn = 100\nA_cpu = sprand(T, n, n, 0.01)\nA_cpu = A_cpu' * A_cpu + I\nb_cpu = rand(T, n)\n\nA_gpu = CuSparseMatrixCSR(A_cpu)\nb_gpu = CuVector(b_cpu)\nx_gpu = similar(b_gpu)\n\nsolver = CudssSolver(A_gpu, \"HPD\", 'F')\n\n# Provide a user permutation\npermutation = amd(A_cpu) |> Vector{Cint}\ncudss_set(solver, \"user_perm\", permutation)\n\ncudss(\"analysis\", solver, x_gpu, b_gpu)\ncudss(\"factorization\", solver, x_gpu, b_gpu)\ncudss(\"solve\", solver, x_gpu, b_gpu)\n\nr_gpu = b_gpu - CuSparseMatrixCSR(A_cpu) * x_gpu\nnorm(r_gpu)","category":"page"},{"location":"options/#Hybrid-mode","page":"Options","title":"Hybrid mode","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = Float64\nn = 100\nA_cpu = sprand(T, n, n, 0.01)\nA_cpu = A_cpu + A_cpu' + I\nb_cpu = rand(T, n)\n\nA_gpu = CuSparseMatrixCSR(A_cpu)\nb_gpu = CuVector(b_cpu)\nx_gpu = similar(b_gpu)\n\nsolver = CudssSolver(A_gpu, \"S\", 'F')\n\n# Use the hybrid mode (host and device memory)\ncudss_set(solver, \"hybrid_mode\", 1)\n\ncudss(\"analysis\", solver, x_gpu, b_gpu)\n\n# Minimal amount of device memory required in the hybrid memory mode.\nnbytes_gpu = cudss_get(solver, \"hybrid_device_memory_min\")\n\n# Device memory limit for the hybrid memory mode.\n# Only use it if you don't want to rely on the internal default heuristic.\ncudss_set(solver, \"hybrid_device_memory_limit\", nbytes_gpu)\n\ncudss(\"factorization\", solver, x_gpu, b_gpu)\ncudss(\"solve\", solver, x_gpu, b_gpu)\n\nr_gpu = b_gpu - CuSparseMatrixCSR(A_cpu) * x_gpu\nnorm(r_gpu)","category":"page"},{"location":"#Home","page":"Home","title":"CUDSS.jl documentation","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CUDSS.jl is a Julia interface to the NVIDIA cuDSS library. NVIDIA cuDSS provides three factorizations (LDU, LDLᵀ, LLᵀ) for solving sparse linear systems on GPUs. For more details on using cuDSS, refer to the official cuDSS documentation.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> ]\npkg> add CUDSS\npkg> test CUDSS","category":"page"},{"location":"generic/#LLᵀ-and-LLᴴ","page":"Generic API","title":"LLᵀ and LLᴴ","text":"","category":"section"},{"location":"generic/#LinearAlgebra.cholesky-Union{Tuple{CuSparseMatrixCSR{T, Int32}}, Tuple{T}} where T<:Union{Float32, Float64, ComplexF64, ComplexF32}","page":"Generic API","title":"LinearAlgebra.cholesky","text":"solver = cholesky(A::CuSparseMatrixCSR{T,Cint}; view::Char='F')\n\nCompute the LLᴴ factorization of a sparse matrix A on an NVIDIA GPU. The type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\nInput argument\n\nA: a sparse Hermitian positive definite matrix stored in the CuSparseMatrixCSR format.\n\nKeyword argument\n\n*view: A character that specifies which triangle of the sparse matrix is provided. Possible options are L for the lower triangle, U for the upper triangle, and F for the full matrix.\n\nOutput argument\n\nsolver: Opaque structure CudssSolver that stores the factors of the LLᴴ decomposition.\n\n\n\n\n\n","category":"method"},{"location":"generic/#LinearAlgebra.cholesky!-Union{Tuple{T}, Tuple{CudssSolver{T}, CuSparseMatrixCSR{T, Int32}}} where T<:Union{Float32, Float64, ComplexF64, ComplexF32}","page":"Generic API","title":"LinearAlgebra.cholesky!","text":"solver = cholesky!(solver::CudssSolver{T}, A::CuSparseMatrixCSR{T,Cint})\n\nCompute the LLᴴ factorization of a sparse matrix A on an NVIDIA GPU, reusing the symbolic factorization stored in solver. The type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\n\n\n\n\n","category":"method"},{"location":"generic/","page":"Generic API","title":"Generic API","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = ComplexF64\nR = real(T)\nn = 100\np = 5\nA_cpu = sprand(T, n, n, 0.01)\nA_cpu = A_cpu * A_cpu' + I\nB_cpu = rand(T, n, p)\n\nA_gpu = CuSparseMatrixCSR(A_cpu |> triu)\nB_gpu = CuMatrix(B_cpu)\nX_gpu = similar(B_gpu)\n\nF = cholesky(A_gpu, view='U')\nX_gpu = F \\ B_gpu\n\nR_gpu = B_gpu - CuSparseMatrixCSR(A_cpu) * X_gpu\nnorm(R_gpu)\n\n# In-place LLᴴ\nd_gpu = rand(R, n) |> CuVector\nA_gpu = A_gpu + Diagonal(d_gpu)\ncholesky!(F, A_gpu)\n\nC_cpu = rand(T, n, p)\nC_gpu = CuMatrix(C_cpu)\nldiv!(X_gpu, F, C_gpu)\n\nR_gpu = C_gpu - ( CuSparseMatrixCSR(A_cpu) + Diagonal(d_gpu) ) * X_gpu\nnorm(R_gpu)","category":"page"},{"location":"generic/","page":"Generic API","title":"Generic API","text":"note: Note\nIf we only store one triangle of A_gpu, we can also use the wrappers Symmetric and Hermitian instead of using the keyword argument view in cholesky. For real matrices, both wrappers are allowed but only Hermitian can be used for complex matrices.","category":"page"},{"location":"generic/","page":"Generic API","title":"Generic API","text":"H_gpu = Hermitian(A_gpu, :U)\nF = cholesky(H_gpu)","category":"page"},{"location":"generic/#LDLᵀ-and-LDLᴴ","page":"Generic API","title":"LDLᵀ and LDLᴴ","text":"","category":"section"},{"location":"generic/#LinearAlgebra.ldlt-Union{Tuple{CuSparseMatrixCSR{T, Int32}}, Tuple{T}} where T<:Union{Float32, Float64, ComplexF64, ComplexF32}","page":"Generic API","title":"LinearAlgebra.ldlt","text":"solver = ldlt(A::CuSparseMatrixCSR{T,Cint}; view::Char='F')\n\nCompute the LDLᴴ factorization of a sparse matrix A on an NVIDIA GPU. The type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\nInput argument\n\nA: a sparse Hermitian matrix stored in the CuSparseMatrixCSR format.\n\nKeyword argument\n\n*view: A character that specifies which triangle of the sparse matrix is provided. Possible options are L for the lower triangle, U for the upper triangle, and F for the full matrix.\n\nOutput argument\n\nsolver: Opaque structure CudssSolver that stores the factors of the LDLᴴ decomposition.\n\n\n\n\n\n","category":"method"},{"location":"generic/#LinearAlgebra.ldlt!-Union{Tuple{T}, Tuple{CudssSolver{T}, CuSparseMatrixCSR{T, Int32}}} where T<:Union{Float32, Float64, ComplexF64, ComplexF32}","page":"Generic API","title":"LinearAlgebra.ldlt!","text":"solver = ldlt!(solver::CudssSolver{T}, A::CuSparseMatrixCSR{T,Cint})\n\nCompute the LDLᴴ factorization of a sparse matrix A on an NVIDIA GPU, reusing the symbolic factorization stored in solver. The type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\n\n\n\n\n","category":"method"},{"location":"generic/","page":"Generic API","title":"Generic API","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = Float64\nR = real(T)\nn = 100\np = 5\nA_cpu = sprand(T, n, n, 0.05) + I\nA_cpu = A_cpu + A_cpu'\nB_cpu = rand(T, n, p)\n\nA_gpu = CuSparseMatrixCSR(A_cpu |> tril)\nB_gpu = CuMatrix(B_cpu)\nX_gpu = similar(B_gpu)\n\nF = ldlt(A_gpu, view='L')\nX_gpu = F \\ B_gpu\n\nR_gpu = B_gpu - CuSparseMatrixCSR(A_cpu) * X_gpu\nnorm(R_gpu)\n\n# In-place LDLᵀ\nd_gpu = rand(R, n) |> CuVector\nA_gpu = A_gpu + Diagonal(d_gpu)\nldlt!(F, A_gpu)\n\nC_cpu = rand(T, n, p)\nC_gpu = CuMatrix(C_cpu)\nldiv!(X_gpu, F, C_gpu)\n\nR_gpu = C_gpu - ( CuSparseMatrixCSR(A_cpu) + Diagonal(d_gpu) ) * X_gpu\nnorm(R_gpu)","category":"page"},{"location":"generic/","page":"Generic API","title":"Generic API","text":"note: Note\nIf we only store one triangle of A_gpu, we can also use the wrappers Symmetric and Hermitian instead of using the keyword argument view in ldlt. For real matrices, both wrappers are allowed but only Hermitian can be used for complex matrices.","category":"page"},{"location":"generic/","page":"Generic API","title":"Generic API","text":"S_gpu = Symmetric(A_gpu, :L)\nF = ldlt(S_gpu)","category":"page"},{"location":"generic/#LU","page":"Generic API","title":"LU","text":"","category":"section"},{"location":"generic/#LinearAlgebra.lu-Union{Tuple{CuSparseMatrixCSR{T, Int32}}, Tuple{T}} where T<:Union{Float32, Float64, ComplexF64, ComplexF32}","page":"Generic API","title":"LinearAlgebra.lu","text":"solver = lu(A::CuSparseMatrixCSR{T,Cint})\n\nCompute the LU factorization of a sparse matrix A on an NVIDIA GPU. The type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\nInput argument\n\nA: a sparse square matrix stored in the CuSparseMatrixCSR format.\n\nOutput argument\n\nsolver: an opaque structure CudssSolver that stores the factors of the LU decomposition.\n\n\n\n\n\n","category":"method"},{"location":"generic/#LinearAlgebra.lu!-Union{Tuple{T}, Tuple{CudssSolver{T}, CuSparseMatrixCSR{T, Int32}}} where T<:Union{Float32, Float64, ComplexF64, ComplexF32}","page":"Generic API","title":"LinearAlgebra.lu!","text":"solver = lu!(solver::CudssSolver{T}, A::CuSparseMatrixCSR{T,Cint})\n\nCompute the LU factorization of a sparse matrix A on an NVIDIA GPU, reusing the symbolic factorization stored in solver. The type T can be Float32, Float64, ComplexF32 or ComplexF64.\n\n\n\n\n\n","category":"method"},{"location":"generic/","page":"Generic API","title":"Generic API","text":"using CUDA, CUDA.CUSPARSE\nusing CUDSS\nusing LinearAlgebra\nusing SparseArrays\n\nT = Float64\nn = 100\nA_cpu = sprand(T, n, n, 0.05) + I\nb_cpu = rand(T, n)\n\nA_gpu = CuSparseMatrixCSR(A_cpu)\nb_gpu = CuVector(b_cpu)\n\nF = lu(A_gpu)\nx_gpu = F \\ b_gpu\n\nr_gpu = b_gpu - A_gpu * x_gpu\nnorm(r_gpu)\n\n# In-place LU\nd_gpu = rand(T, n) |> CuVector\nA_gpu = A_gpu + Diagonal(d_gpu)\nlu!(F, A_gpu)\n\nc_cpu = rand(T, n)\nc_gpu = CuVector(c_cpu)\nldiv!(x_gpu, F, c_gpu)\n\nr_gpu = c_gpu - A_gpu * x_gpu\nnorm(r_gpu)","category":"page"}]
}
